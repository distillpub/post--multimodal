<!DOCTYPE html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Multimodal Representation in the Multimodal Model</h1>
  </d-title>

  <d-article>


    <d-contents>
      <nav class="toc figcaption">
        <h4>Contents</h4>
        <!--<div><a href="#introduction">Introduction</a></div>-->
        <div><a href="#">Introduction</a></div>
        <div><a href="#">Into the Multimodal Mind</a></div>
        <ul>
          <li><a href="#">Identity Recognition</a></li>
          <li><a href="#">Regional Neurons</a></li>
          <li><a href="#">Emotion Neurons</a></li>
          <li><a href="#miscellaneous-neurons">Miscellaneous Neurons</a></li>
        </ul>
        <div><a href="#">Using Abstractions</a></div>
        <ul>
          <li><a href="#">Understanding language</a></li>
          <li><a href="#">Emotional Intelligence</a></li>
        </ul>
        <div><a href="#">Typographic Attacks</a></div>
        <div><a href="#">The Mechanics of Abstraction</a></div>
        <ul>
          <li><a href="#">Universal Features and Circuits</a></li>
          <li><a href="#">Task-specific Features</a></li>
          <li><a href="#">The Mechanics of Abstraction</a></li>
        </ul>
        <div><a href="#">Conclusion</a></div>
        <div><a href="#">Appendix: Methodological Details</a></div>
      </nav>
      <div class="toc-line"></div>
    </d-contents>



  <p>In 2005, a letter published in Nature described human neurons responding to specific people,  as well as landmarks <d-cite key="quiroga2005invariant"></d-cite>. The exciting thing wasn’t just that they selected for particular people, such as Jennifer Anniston or Halle Berry,
    <d-footnote>previous results had reported units that respond to particular faces []</d-footnote>
  but that they did so regardless of whether they were shown images, drawings, or even images of the person’s name. The neurons were <i>multimodal</i>. As the lead author would put it: "You are looking at the far end of the transformation from metric, visual shapes to conceptual… information."
    <d-footnote>Quiroga's full quote, from <a href='https://www.newscientist.com/article/dn7567-why-your-brain-has-a-jennifer-aniston-cell/'>New Scientist</a> reads: "I think that’s the excitement to these results. You are looking at the far end of the transformation from metric, visual shapes to conceptual memory-related information. It is that transformation that underlies our ability to understand the world. It’s not enough to see something familiar and match it. It’s the fact that you plug visual information into the rich tapestry of memory that brings it to life." We elided the portion discussing memory since it was less relevant.</d-footnote></p>

  <p>We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters,
  such as Donald Trump,
  Lady Gaga, and Spiderman.
    <d-footnote>It’s important to note that the vast majority of people these models recognize don’t have a specific neuron, but instead are represented by a combination of neurons. Often, the contributing neurons are conceptually related. For example, the Trump neuron fires (albeit more weakly) for Mike Pence, contributing to representing him.</d-footnote>
    <d-footnote>Some of the figures we found neurons for are divisive. It should go without saying that having a dedicated neuron simply reflects the prominence of these figures in the training data, collected in [2019?]. Being divisive increases this prominence, because divisive figures are more likely to be extensively discussed on the Internet. In the case of the Donald Trump neuron, it seems likely there would have also been a Hillary Clinton neuron if data had been collected in 2016 instead. (There are other neurons which respond to Hillary Clinton in addition to other topics.) </d-footnote>
    <d-footnote>Not only do our neurons match Quiroga et al in firing for the same feature in photo, drawing, and text form, but some of the specific neurons are strikingly similar to those they describe. The Donald Trump neuron we found might be seen as similar to Quiroga et al’s Bill Clinton neuron. A Star Wars neuron we find seems analogous to a biological Star Wars neuron described Quiroga et al’s follow up paper []. And although we don’t find an exact Jennifer Anniston neuron, we do find a neuron for the TV show “Friends” which fires for her.</d-footnote>
  Like the biological multimodal neurons, these neurons respond to the same subject in photographs, drawings, and images of text:
</p>

<figure style="display: grid; grid-column-gap: 64px; grid-row-gap: 16px; grid-template-columns: repeat(2, auto); max-width: 600px;">
  <div class='figcaption'>Just as the biological Halle Berry neuron responds to photos, drawings, and text...</div>
  <div class='figcaption'>... multimodal neurons in artificial neural networks activate for photos, cartoons, and text as well.</div>
  <img src="images/IntroHalleBerry.png" />
  <div>
    <img src="images/IntroTrump.png" />
    <div class="microscope-button" data-unit="89" />
  </div>
</figure>


<div class='todo'><b>TODO:</b> Trump neuron should be replaced or supplemented here once we have more dataset examples and find drawing/cartoon examples for other neurons.</div>

<p>
  But these people-detecting neurons only scratch the surface of highly abstract neurons we found. We also find features coding for emotions, regions of the world, fictional settings, and much more.
  These neurons generalize in highly abstract ways beyond photo/drawing/text multimodality. For example, the regional neurons respond to the relevant regions on a world map, dominant ethnicities, local script, the names of countries and cities, and much more.
</p>


<figure id="feature-overview" class='fullscreen-diagram'>
</figure>


<p>
We find these multimodal neurons in the recent CLIP models [], although it's possible similar undiscovered multimodal neurons may exist in earlier models.
A CLIP model consists of both a ResNet vision model and a Transform language model, trained to align pairs of images and text from the internet using a contrastive loss.
  <d-footnote>The authors also kindly shared an alternative version from earlier experiments, where the training objective was an autoregressive language modelling cost, instead of a contrastive objective. The features seem pretty similar.</d-footnote>
There are actually several CLIP models of varying sizes; we find evidence of multimodal neurons in all of them, but focus on studying the mid-sized "4x" model.
  <d-footnote>We found it challenging to make feature visualization work on thee largest CLIP models. The reasons why remain unclear.</d-footnote>
We refer readers to their paper for a detailed description.

</p>

<p>
  We focus on vision model, so when we talk about a multimodal neuron responding to text we mean the model "reading" <i>images</i> of text.
  <d-footnote>
  The alignment with the text side of the model might be seen as an additional form of multimodality, perhaps analogous to a human neuron responding to hearing a word rather than seeing it.
  </d-footnote>
In some ways, it seems natural for contrastive training of vision and language to create these abstract visual features. We expect word embeddings and language models to learn abstract "topic" features []. Arguably, we’re just seeing the vision model align with the native abstraction of language models.
<d-footnote>Many researchers are interested in “grounding” language models by training them on tasks involving another domain, in the hope of them learning a more real world understanding of language. The abstract features we find in vision models can be seen as a kind of “inverse grounding”: vision taking on more abstract features by connection to language.</d-footnote>
<!--Should add some kind of discussion to section 1  if we want to include the following:
<d-footnote>This includes some of the classic kinds of bias we see in word embeddings: for example, a feature that responds to rastered images of the following words: housewives, kathy, britney, jennifer, elizabeth, amanda, madonna, daughters, pregnancy, pharmaceutical, pharmaceuticals, amy, tiffany, barbara, daughter, healthcare.</d-footnote>-->
</p>


    <p>
      We begin this article with a <a>preliminary analysis</a> of some of the
      features that exist in these multimodal models. From there, we’ll explore
      <a>how those features are used</a> to accomplish the models goal of
      aligning images and text. This leads us to discover that these models are
      vulnerable to a kind of <a>“typographic attack”</a> where adding the right
      text to images can cause them to be systematically misclassified.
    </p>

    <figure id="intro-attack-demo"></figure>

    <p>
      Finally, we’ll explore some of the intermediary features and circuits that
      build up abstract features. These explorations will require a few novel
      methods which we discuss in detail in an appendix 1.
    </p>

    <br />
    <hr />
    <br />

    <h2>Into the Multimodal Mind</h2>

    <h3>Identity Recognition</h3>

    <p>
      A lot of captioning is about having cultural context. Imagine trying to
      caption popular pictures from a foreign culture. Even though you find
      image processing skills like object and facial recognition effortless,
      you’d struggle. Knowing about sports in general will barely help you
      caption pictures from the stadiums of games you don’t recognize, and you
      may even need to know specific teams and players to get the captions
      right. Similarly, it's almost impossible to caption someone giving a
      speech -- some of the most photographed scenes on the internet -- if you
      don't know who's talking and what they usually talk about. With this in
      mind, perhaps it’s unsurprising that the model invests significant
      capacity in understanding the most prominent people in popular
      culture.[footnote?]
    </p>

    <p>
      There's a Hitler neuron that activates not just to the historical figure's
      face and body, but to symbols of the Nazi party, relevant historical
      documents, books related to concentration camps, and more weakly activates
      for loosely related concepts like German beer and sausages. Feature
      visualizations reveal swastikas and Hitler seemingly doing a Nazi
      salute.[foot:fv,facet] There’s a Jesus Christ neuron that fires for
      Christian symbols like crosses and crowns of thorns, paintings of Jesus,
      his written name, and feature visualization renders baby Jesus being held
      in the arms of the Virgin Mary. There’s a Spiderman neuron that activates
      not just to the masked hero in action, but also knows his secret identity,
      Peter Parker, his nemesis Venom, as well as text and drawings of each of
      these characters from a breadth of Spiderman movies and comic books over
      the last half-century.
    </p>

    <p>
      Whether someone earns a dedicated neuron depends on how common they are in
      the dataset and how far they are conceptually from the nearest neuron. For
      all but one person, whether they get a dedicated neuron depends on the
      model details and training run. In some models Hitler has his own neuron,
      but in others he's a mere facet of a broader racism and hate-speech
      neuron. It seems that the marginal value of keeping track of Hitler
      separate from general hate -speech is close to the cost of giving up a
      precious neuron.
    </p>

    <p>
      The only person we've seen a dedicated neuron for in every model is Donald
      Trump. Presumably, this reflects his prevalence in 2019 online discourse
      when the dataset was collected, but this also tells us that an image
      containing Trump has a meaningfully different caption than one containing
      a generic political figure. This neuron is by far the most sophisticated
      neuron we've studied, responding to the politician in a wide variety of
      poses and settings, effigies and caricatures in many artistic mediums,
      people he’s worked closely alongside such as Mike Pence and Steve Bannon,
      and Trumpian political symbols such as Make America Great Again hats and
      “The Wall”.
    </p>

    [positive activations]

    <p>
      In addition to concepts that cause the neuron to activate most positively,
      we can look at a neuron's pre-ReLU activation to see which concepts the
      neuron anti-associates with Trump. Across the dataset, the strongest
      negative activations reliably correspond to hip hop and rap musicians like
      Eminem, video games like Fortnite, black rights activists like Martin
      Luther King Jr., and LGBT symbols like rainbow flags.
    </p>

    [people]

    <p>
      To better understand this sophisticated neuron better we studied what
      kinds of images cause different levels of activation. We automatically
      collected about 650 images from the dataset that fall into different
      activation ranges and labelled them by hand into a number of high level
      categories, blind to how much an image made the neuron fire as we labelled
      it. By plotting all of these image's labels against their activation we
      get an estimate of the conditional probability of a label at a given
      activation value.
    </p>

    <figure id="people-handlabeled" class="fullscreen-diagram"></figure>

    <p>
      We can also measure how the Trump neuron corresponds to different people.
      In the following experiment, we query Google Images for X images with the
      term “Y giving a speech” for a variety of people and plot the conditional
      probability of an image being from each person at a given activation.
      Since most activations are weakly negative, corresponding to relatively
      uninteresting parts of images like sky and background, the weakly negative
      conditional probability is about equal for all people. But if we look at
      higher activations, we see they are all pictures of Trump, and if we look
      at strongest negative activations, they are Nelson Mandela, Gandhi, and
      Martin Luther King Jr.
    </p>

    <figure id="people-trumppeople" class="fullscreen-diagram"></figure>

    <p>
      As far as we’re aware, this is the first time that detailed knowledge of
      individual people has been found in a neural network, and it is
      reminiscent of the famous Halle Berry and Jennifer Anniston neurons result
      from neuroscience. One of the most striking aspects of their work was that
      the neurons were multimodal, responding not just to the actress's face but
      her name. Neurons in this model demonstrate this same multimodality, but
      push it further to a much larger space of modalities surrounding the
      person. In this sense, while calling the neuron a person detector seems
      reasonable, it is more like a landscape of ideas related to a high level
      concept, and the person is simply the tallest peak.
    </p>

    <p>
      We also find "regional neurons" which respond to a fusion of features
      roughly associated with a geographic region: country and city names,
      distinctive architecture, prominent public figures, faces of the most
      common ethnicity, distinctive clothing, wildlife, and local script (if not
      roman alphabet). If shown a world map, even without labels, these neurons
      fire selectively for the relevant region on the map.
    </p>

    <p>
      Most often, these correspond to a continent (e.g. Africa, Australia,
      Europe), clusters of countries (e.g. Islamic Countries, India/Pakistan,
      East Asia, Latin America), or individual countries (e.g. USA, UK, China,
      Japan). We also observe neurons for larger geographical features, such as
      a northern hemisphere neuron (which responds to bears, moose, coniferous
      forest, and the entire northern hemisphere on a map), and a "tropical"
      neuron for regions along the equator.
    </p>

    <figure id="regional-neurons" class="fullscreen-diagram"></figure>
    <p>
      [Figure: similar to NMF diagram in building blocks, associate neurons with
      colors, then highlight region on world map with color, and also show
      faceted feature visualization for each neuron. Possibly show dataset
      examples as well]
    </p>

    <p>
      Again, many neurons of the same neurons exist across models. In
      particular, Africa, Australia, United States, and China neurons seem to
      form very consistently. [Table of region-detecting neurons across models.
      Perhaps subject as row, model as col, and face-facet feature vis as entry
      in table when found. Perhaps include count and sort by number of
      occurrences of figure?]
    </p>

    <p>
      In addition to these regional neurons, we find that many other neurons
      appear to have geographic information baked in, firing weakly for regions
      on a world map related to them. For example, we noticed that a coffee
      neuron fires for Brazil, where a significant amount of coffee is made, and
      a lion/tiger neuron fires for the parts of Africa and Asia where lions and
      tigers are found, a common human mistake! We're hesitant to read too much
      into this -- there's plenty of neurons which activate a bit for a world
      map without any obvious relationship to the topic -- but it is an
      interesting phenomenon.
    </p>

    <h3>The Emotion Family of Neurons</h3>

    <p>
      The task of captioning Internet images requires emotional intelligence, as
      small changes in a person's expression can radically change the meaning of
      a photo. We've found dozens of emotion detecting neurons in the multimodal
      model, each responding to the concept of a single emotion across a wide
      range of modalities, from body language and facial expressions, to text,
      to different species and cartoons, and even landscapes and the insides of
      buildings that evoke a feeling. There are neurons that respond to common
      feelings like anger and happiness, and as well as more nuanced emotions
      like feeling destroyed, aroused, honored, or silly.
    </p>
    <figure id="emotions-intro" class="fullscreen-diagram"></figure>
    <p>
      These emotions neurons are versatile and highly multimodal, spanning
      facial expressions across age, cartoon characters, relevant text, and even
      species. The surprise neuron activates even when the majority of the face
      is obscured. It responds to slang like "OMG!" and "WTF", and text feature
      visualization produces similar words of shock and surprise.
    </p>
    <figure id="emotions-surprise" class="fullscreen-diagram"></figure>
    <p>
      Other neurons learn to detect emotions as part of a broader concept. At
      first glance neuron xyz responds to the concept of sexuality, activating
      in response to visual pornographic content, sexual slang, and the names of
      adult websites. However, using a facial facet with feature visualization,
      we see it also corresponds to the emotion of arousal, rendering a balding
      male with an exaggerated aroused expression. Similarly, we find a question
      mark neuron hides … In each of these cases, the neuron indeed detects an
      emotion, but since it is only part of a more abstract concept, we need
      facets to see it.
    </p>
    <figure id="emotions-minor" class="fullscreen-diagram"></figure>
    <p>
      On the other extreme, some neurons respond simply to specific body and
      facial expressions, like the silly expression neuron. It activates most to
      the internet-born duckface expression, and peace signs, and both words
      show up in its text feature visualization.
    </p>
    <figure id="emotions-duckface" class="fullscreen-diagram"></figure>
    <p>
      We're excited at the promise of emotion neurons to benefit social studies
      that need algorithmic access to emotion detectors. These neurons could
      help people understand the emotional content of a long video, either by
      coloring the video's scrubber based on the emotions at each time, or by
      showing an Activation Atlas of the scenes of a movie based on emotion to
      study cinematography in a new way. These emotions could also be useful for
      studying how expression changes over time. With a dataset of selfies over
      the last decade with their location tagged, perhaps one could better
      understand birth and spread of expressions like the duckface across
      different cultures and geography.
    </p>
    <h4>Mental Health Neuron</h4>
    <p>
      One neuron that doesn't represent a single emotion but rather a high level
      category of emotions is the mental illness neuron, which is separate from
      a different physical illness neuron. It responds to both body language,
      facial expressions, and text corresponding to a variety of emotions like
      anxiety, depression, and loneliness. It also responds to images and text
      of illegal drugs and medicine, and psychological words like "mental" and
      "psychology".
    </p>

    <p>
      To better understand the behavior of this neuron we collected more than
      300 dataset examples at a variety of activations and labelled them by hand
      into categories the neuron seemed to respond to. During the labeling
      process we did not have access to the activation. We can use this
      hand-labelled data to estimate the conditional probability of a category
      at a given activation.
    </p>

    <p>
      We see that the neuron activates most strongly to mental health disorders,
      but also to the broader concept of psychology. We can also look at the
      pre-ReLU activations to see what the neuron negatively responds to. It has
      a weak negative response to concepts we may expect to be negatively
      correlated with mental health disorders, like pets, sports, and travel.
      Many of its strongest negative activations are to exercise, sporting
      events, and music related ideas like albums and concerts.
    </p>
    <figure id="emotions-mentalhealth" class="fullscreen-diagram"></figure>

    <h3 id="miscellaneous-neurons">Miscellaneous Neurons</h3>

    <p>
      <b>Person detectors.</b>
    </p>
    <p>
      <b>Regional neurons.</b>
    </p>
    <p>
      <b>Emotion neurons.</b>
    </p>
    <div class="todo">
      <b>TODO(@csvoss)</b>: These hyperlinks (on neurons below) are right ... but are the images?
    </div>
    <p>
      <b>Person trait neurons.</b> These neurons detect gender, age, and ethnicity, but also facial features like moustaches.
    </p>
    <figure
      class='small-neuron-grid'
      data-neurons='839,2231,2518,320,92'
      data-titles='elderly,teenage,female,male,moustache'
      data-facets='any,logo,face,text'
      data-models='4x,4x,4x,4x,4x'
      data-layers='image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0'
    ></figure>
    <p class="figcaption">
      Feature visualization shown along with logo, face, and text facets.
    </p>
    <p>
      <b>Image type neurons.</b> These neurons detect different ways an image might be drawn, rendered, or photographed.
    </p>
    <figure
      class='small-neuron-grid'
      data-neurons='1308,359,319,436'
      data-titles="selfie,portrait,cartoon,child's drawing"
      data-facets='any'
      data-models='4x,rn101,rn101,rn101'
      data-layers='image_block_4_5_Add_6_0,image_block_4_1_add_3_0,image_block_4_1_add_3_0,image_block_4_2_add_3_0'
    ></figure>
    <p>
      For a genre-specific example of image types, RN101 distinguishes among these three different types of medical content. Although they share an underlying topic (medicine and physiology), nonetheless these three different image types each get their own dedicated neuron!
    </p>
    <figure
      class='small-neuron-grid'
      data-neurons='362,337,108'
      data-titles='microscope photos,medical textbook,physicians in scrubs'
      data-facets='any'
      data-models='rn101,rn101,rn101'
      data-layers='image_block_4_2_add_3_0,image_block_4_1_add_3_0,image_block_4_1_add_3_0'
    ></figure>
    <p>
      <b>Image feature neurons.</b> These neurons detect different miscellaneous features that a photo might contain: photobombs and bunny ears, the heads of people seated in front of you at a lecture, Photoshopped modifications, and more.
    </p>
    <figure
      class='small-neuron-grid'
      data-neurons='1640,127,2272'
      data-titles='altered image,seated at a lecture,bunny ears'
      data-facets='any,logo,face,indoor,text'
      data-models='4x,4x,4x'
      data-layers='image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0'
    ></figure>
    <p class="figcaption">
      Feature visualization shown along with logo, face, and indoor facets.
    </p>
    <!-- Can't find yet: photo bomb, bunny ears, group hug -->
    <p>
      <b>Counting neurons.</b> These neurons detect duplicates of the same person or thing, and can distinguish them by their count. For example, the "trios" neuron might fire for a trio of friends taking a photo together, whereas the "pairs or fours" neuron might fire for pairs of shoes, pairs of cookies, and pairs of people.
    </p>
    <figure
      class='small-neuron-grid'
      data-neurons='17,202,310'
      data-titles='trios,pairs or fours,many'
      data-facets='any,logo,face'
      data-models='4x,4x,4x'
      data-layers='image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0'
    ></figure>
    <p class="figcaption">
      Feature visualization shown along with logo and face facets.
    </p>
    <p>
      <b>Holiday neurons.</b> These neurons recognize the names, decorations, and traditional trappings around various holidays.
    </p>
    <!-- Couldn't get to work: happy/god – 36 / 4x / image_block_4_5_Add_6_0 -->
    <figure
      class='small-neuron-grid'
      data-neurons='776,1326,1204,2439'
      data-titles='birthday,christmas,easter,halloween'
      data-facets='any,logo,face,indoor,text'
      data-models='4x,4x,4x,4x'
      data-layers='image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0'
    ></figure>
    <p class="figcaption">
      Feature visualization shown along with logo, face, indoor, and text facets.
    </p>
    <p>
      <b>Fiction neurons.</b> These neurons represent characters and concepts from within a particular fictional universe.
      Harry Potter, Avengers, 924 Pokemon/Nintendo
    </p>
    <p>
      <b>Typographic neurons.</b> Surprisingly, despite being able to “read” words and map them to semantic features, the model keeps a handful of more typographic features (eg. detectors for “con-”, “-ing”) in its high-level representation. We suspect these may help the model pass words it can’t read to the captioning side, like a child spelling out a word they don’t know.

      <!-- -tor: contrastive_v1/image_block_4_2_Add_6_0/707, phil-: contrastive_v1/image_block_4_2_Add_6_0/843 -->
    </p>
    <figure
      class='small-neuron-grid'
      data-neurons='2504,656,444,2524,371,399'
      data-titles='un-,cons-,-oo-,-ing,you,j-'
      data-facets='text'
      data-models='4x,4x,4x,4x,4x,4x'
      data-layers='image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0,image_block_4_5_Add_6_0'
    ></figure>
    <p class="figcaption">
      Text facets of the given neurons.
    </p>
    <p>
      <b>Brightness Gradient Neurons:</b> Another surprising cluster of features are lighting gradient features, which detect a better lit object or image portion than its surroundings. Why does the model have such low-level features in its high-level representation? Perhaps they help inform which portions of the image are more likely to be the subjects of captions.
    </p>

    <h3>Feature properties</h3>

    <p>So far, we’ve looked at particular neurons to give a sense of the kind of features that exist in CLIP models. It's worth noting several properties that either warrant emphasis or might be missed in the discussion of individual features:</p>

    <p><b>Multimodality / Abstraction:</b>  As we’ve seen in the previous sections, most CLIP neurons are multimodal and abstract, responding to the same concept across forms such as photos, drawings, maps, images of text, and more.</p>

    <p><b>Image-Based Word Embedding:</b> Despite being a vision model, one can produce “word embeddings” with the visual CLIP model by rastering words into images and then feeding these images into the model. Like normal word embeddings, the nearest neighbors of words tend to be semantically related.
      <d-footnote>A comparison of CLIPs image-based word embedding to examples in Collobert <d-cite key="collobert2011natural"></d-cite>’s example:
      <span class='figcaption' style='display: grid; grid-template-columns: auto auto auto; grid-gap: 12px; margin-top: 18px;'>
      <span style='font-weight: bold; border-bottom: 1px solid #CCC;'>Original <br>Word</span>
      <span style='font-weight: bold; border-bottom: 1px solid #CCC;'>Nearest Neighbors <br>Collobert<d-cite key="collobert2011natural"></d-cite> embeddings</span>
      <span style='font-weight: bold; border-bottom: 1px solid #CCC;'>Nearest Neighbors <br>CLIP image-based embeddings</span>
      <span>France</span>
      <span>Austria, Belgium, Germany, Italy, Greece, Sweden, Norway, Europe, Hungary, Switzerland</span>
      <span>French, Francis, Paris, Les, Des, Sans, Le, Pairs, Notre, Et</span>
      <span>Jesus</span>
      <span>God, Sati, Christ, Satan, Indra, Vishnu, Ananda, Parvati,  Grace</span>
      <span>Christ, God, Bible, Gods, Praise, Christians, Lord, Christian, Gospel, Baptist</span>
      <span>xbox</span>
      <span>Amiga, Playstation, Msx, Ipod, Sega, Ps#, Hd, Dreamcast, Geforce, Capcom</span>
      <span>Xbox, Gaming, Nintendo, Playstation, Console, Box, Lightbox, Sony, Sega, Games, Microsoft</span>
      </span></d-footnote>
    Word arithmetic <d-cite key="mikolov2013linguistic"></d-cite> such as <br><span style='display: inline-block; margin: 12px;'><i>V(Img(</i>“King”<i>)) <span style='margin: 4px'>-</span> V(Img(</i>“Man”<i>)) <span style='margin: 4px'>+ </span>V(Img(</i>“Woman”<i>)) <span style='margin: 4px'>=</span> V(Img(</i>“Queen”<i>))</i></span><br> work in some cases if we mask non-semantic lexicographic neurons (eg. “-ing” detectors).
    It seems likely that mixed arithmetic of words and images should be possible.
    </p>

    <p><b>Limited Multilingual Behavior:</b> Although CLIP’s training data was filtered to be English, many features exhibit limited multilingual responsiveness. For example, a “positivity” neuron (4x:36) responds to images of English “Thank You”, French “Merci”, German “Danke”, and Spanish “Gracias,” and also to English “Congratulations”, German “Gratulieren”, Spanish “Felicidades”, and Indonesian “Selamat”. As the example of Indonesian demonstrates, the model can recognize some words from non Romance/Germanic languages. However, we were unable to find any examples of the model mapping words in non-latin script to semantic meanings. It can recognize many scripts (Arabic, Chinese, Japanese, etc) and will activate the corresponding regional neurons, but doesn’t seem to be able to map words in those scripts to their meanings.
      <d-footnote>One interesting question is why the model developed reading abilities in latin alphabet languages, but not others -- was it because more data of that type slipped into the training data, or (the more exciting possibility) because it’s easier to learn a language from limited data if you already know the alphabet?</d-footnote></p>

    <p><b>Bias:</b> Certain kinds of bias seem to be embedded into these representations, similar to classic biases in word embeddings (eg. <d-cite key="bolukbasi2016man"></d-cite>). The most striking examples are likely racial and religious bias. For example, there seems to be a “terrorism/Islam” neuron (4x:1596) which responds to images of words such as “Terrorism”, “Attack”, “Horror”, “Afraid”, and also “Islam”, “Allah”, “Muslim”. This isn’t just an illusion from looking at a single neuron: the image-based word embedding for “Terrorism” has a cosine similarity of 0.98 with “Muslims”. Similarily, an “illegal immigration neuron” (4x:2213) selects for Latin America countries.
    (We’ll see further examples of bias in the next section, when we how these features are used in aligning with captions.)   </p>

    <p><b>Polysemanticity and Conjoined Neurons:</b>  Although we’ve focused on neurons which seem to have a single clearly defined concept they respond to, many CLIP neurons are “polysemantic” <d-cite key="olah2017feature,olah2020zoom"></d-cite>, responding to multiple unrelated features. Unusually, polysemantic neurons in CLIP often have suspicious links between the different concepts they respond to. For example, we observe as <b>Phil</b>adelphia/<b>Phil</b>ipines/<b>Phil</b>ip neuron, as Christm<b>as</b>/<b>As</b>s neuron, and an Ac<b>tor</b>/Velocerap<b>tor</b> neuron. The concepts in these neurons seem “conjoined”, overlapping in a superficial way in one facet, and then generalizing out in multiple directions. We haven’t ruled out the possibility that these are just coincidences, given the large number of facets that could overlap for each concept. But if conjoined features genuinely exist, they hint at new potential explanations of polysemanticity.
      <d-footnote>In the past, when we've observed seemingly polysemantic neurons, we've considered two possibilities: either it is responding to some shared feature of the stimuli, in which case it isn’t really polysemantic, or it is genuinely responding to two unrelated cases. Usually we distinguish these cases with feature visualization. For example, InceptionV1 4e:55 responds to cars and cat heads. One could imagine it being the case that it’s responding to some shared feature -- perhaps cat eyes and car lights look similar. But feature visualization establishes a facet selecting for a globally coherent cat head, whiskers and all, as well as the metal chrome and corners of a car. We concluded that it was genuinely <i>OR(cat, car)</i>.<br><br>
      Conjoined features can be seen as a kind of mid-point between detecting a shared low-level feature and detecting independent cases. Detecting Santa Claus and “turn” are clearly true independent cases, but there was a different facet where they share a low-level feature. <br><br>
      Why would models have conjoined features? Perhaps they’re a vestigial phenomenon from early in training when the model couldn’t distinguish between the two concepts in that facet. Or perhaps there’s a case where they’re still hard to distinguish, such as large font sizes. Or maybe it just makes concept packing more efficient, as in the superposition hypothesis.</d-footnote>
    </p>


    <br />
    <hr />
    <br />

    <h2>Using Abstractions</h2>

    <p>
      It has been hypothesize that the formation of intermediate representions, both in the brain and in neural networks, faciliate "subspace untangling" [] -- a transformation of an input into a vector that has that allows rich queries on relevant task through dot products.
    </p>

    <p>
      This is, indeed, explicitly the goal of CLIP, where linear proces achieve impressive accuracies in imagenet classification [Section []], facial expression detection [], geolocalization, and more. Can we understand the meanings of these directions? What information can be encoded in them? We make a stab at making this concrete by taking a deep dive into one particular task: the Imagenet challenge.
    </p>


    <h3>The Imagenet Challenge</h3>
    <p>
      The IILSRV challenge [](?) uses a subset of the wordnet hierarchy and is an explicit attempt to systematize human knowledge. The synsets are organized as overlapping trees, but a classifier is trained to clasisify the leaves.
    </p>
      Following the methodlogy of Radford [] et all, we train a sparse linear probe on the imagenet labels to understand which neurons are the most informative for the task. We do this with only 3, on average, nonzeros per class, forcing the network to use neurons very sparingly.
    </p>

    <p>
      This model, by most any modern standard, fares poorly, achieving an accuracy of 35%. Our goal, however is to understand how a model as parsimonious as this can do <i>anything</i> at all. As we inspect the model's weights, a clearer picture emerges.
    </p>

    <figure id="hypergraph-device" class="fullscreen-diagram"></figure>

    <p>
      At the highest levels we find a single neuron that represents the split between the living - animal, and the nonliving, that fires for nearly all the animals in the 1000 classes chosen.
    </p>

    <p>
      The animal kingdom itself is split into the domesticated pets and
      wildlife. More conventionally, the animal kingdom is also split into more conventional categories, such as insects, birds and reptiles.
    </p>

    <p>
      We see other classes too that do not correspond neatly to such classes organized by experts, but nevertheless make sense. We see, for example, three neurons that respond to creatures found in different aspects of the ocean/water.
    </p>

    <p>
      The “piggy bank” class in imagenet, for example, can still be obtained by combining neurons that respond to abstract concepts.
    </p>

    <p>
      We arrive at a surprising discovery -- it seems as though the neurons appear to arrange themselves into a taxonomy of classes that appear to mimic, very approximately, the imagenet heirarchy.
    </p>

    <p>
      This gives us a tantalizing insight into the interpretability of neural networks. While there have been attempts to explicitly integrate this information <d-cite key="santurkar2020breeds"></d-cite>, in CLIP this has not been given in any explicit form as a training signal. The fact that it these neurons are scrutible suggests that organizations of high-level concepts of this kind may be a universal feature of learning systems.
    </p>

    <figure id="hypergraph-device" class="fullscreen-diagram"></figure>

    <h3>Understanding language</h3>

    <p>
      The primary consumer of these representations, and what will now be the object of our attention, is the language head of the model. Indeed, augmented with the language head alone, the model is capable of being "programmed" to perform many feats of classification using nothing more than a natural language description of the task.
    </p>

    <p>
      It seems plausible that, just as individual neurons may play key roles in linear probes, they may also encourage the firing of key words or even phrases. Formally, we ask, if we were to perturb the neuron in a positive direction, what kinds of words does this encourage? Conversely, given a word (or a set of words), can we break down the meaning of that word in a crisp set of neurons? To do this, we rely on the following approximation:
    </p>

    <p>
      $$
        \begin{align}
        \mbox{\textbf{cosine}}(F(x+\delta),y) & \approx\mbox{\textbf{cosine}}(F(x)+\nabla F(x)^{T}\delta,y)\\
         & =\mbox{\textbf{cosine}}(F(x)+V\delta,y)\\
         & \approx y^{T}V\delta+\mbox{\textbf{cosine}}(F(x),y)
        \end{align}
      $$
    </p>

    <p>
      Which states that to understand the perturbation due to an image, we can simply look its dot product with the value matrix M. Armed with this insight, we can make these inqueries in a way that is independent of any image - simply by trying to maximize this dot product, either on the language or the image side. We can now use this tool to understand how the model understands emotions.
    </p>

    <h3></h3>

    <h2>Emotional Intelligence</h2>

    <p>

    </p>

    <p>
      Earlier we looked at neurons that detect emotion through a variety of
      modalities such as text, facial expressions, body poses, and sometimes
      even landscapes. But these individual neurons only show us a hint of the
      vast landscape of emotion the model understands, since all emotions are
      slightly different, built from individual neurons working together in
      concert.
    </p>

    <p>
      In fact, seeing this full landscape may be intractable, since emotions in
      this model are inextricably tied to the rest of its knowledge. At one
      extreme of specificity, the model learns expressions that are specific to
      people that appear frequently in the dataset, like Donald Trump or Justin
      Bieber. A more general example is that a neuron that corresponds to
      trophies also responds to the emotion pride. Since every neuron in the
      model may contain an emotion or expression as a facet of a broader
      concept, we need to know where to look.
    </p>

    <p>
      One way is to take inspiration from emotion research, where the names of
      common emotions are sometimes visualized hierarchically on a circle, known
      as a feeling wheel. We can get a list of emotions from a common feeling
      wheel and get the group of neurons in the last hidden layer that
      corresponds to these emotions by taking attribution to the text "I feel
      {emotion}" in the contrastive layer, giving us a vector of neurons for
      each feeling.
    </p>

    <p>
      We can visualize these feelings on an hexagonal atlas<d-footnote
        >This is visually similar to the Activation Atlas, although it has
        several differences in it's implementation detail, such as a lack of
        binning and the hexagonal constraint.</d-footnote
      >, showing the emotion name and facial facet for the corresponding vector
      on each cell. Following the feeling wheel, we can make the atlas easier to
      understand at a glance by applying non-negative matrix factorization (NMF)
      to the vectors and visualizing the groups with color. One difference
      between the feeling wheel and our atlas is that regions of the map can
      smoothly flow into each other rather, reflecting the fact that NMF is
      continuous.
    </p>

    <figure id="emotions-atlas" class="fullscreen-diagram"></figure>

    <p>
      Unlike hand-crafted emotion wheels, we can change easily alter the number
      of components in our factorization. Interestingly, when we use the same
      number of the factors (7) as the feeling wheel we took our list of
      emotions from, they line up neatly with the hand-crafted clusters with one
      exception. Instead of considering "disgusted" a high level category of
      emotions, the model dedicates a factor that roughly corresponds to care or
      love, including valued, loving, lonely, and insignificant in it. It places
      "disgusted" in the anger category, positioned between critical and angry,
      and considers most emotions the original feelings wheel puts under
      disgusted as belonging mostly to either surprised and angry.
    </p>

    <p>
      While the atlas is useful for seeing a general birds-eye view of this set
      of emotions, we can slice these vectors in more granular ways as well.
    </p>

    <p>
      One way is to view all emotions in context of other ones. For instance, in
      psychology emotions are often thought of as existing on the two axes of
      valence, or how good something feels, and arousal, or how energetic it is.
      Depression is low-valence, low-arousal, and excitement is high-valence,
      high-arousal. We can plot each of the emotion attributions above in a
      similar space, using the happy neuron to represent valence and the shocked
      neuron to represent arousal, and largely reconstruct the classic
      valence/arousal plot in psychology. Additionally, we can render a grid of
      facial facets to visualize the space of facial expressions across valence
      and arousal. We've included both in Appendix A.
    </p>

    <p>
      We can also look at emotion attribution vectors one at a time, decomposing
      them into their individual neurons with an interface introduced in The
      Building Blocks of Interpretability [] called a semantic dictionary. We
      see that many of the emotion words from the atlas are in fact composed of
      reasonable and sometimes clever combinations of emotion neurons.
    </p>

    <p>
      For example, the jealousy emotion is composed of success + grumpy. Bored
      is being grumpy and relaxed. Intimate is a soft smile plus heart, minus
      sick. Interested is question mark + heart and inquisitive is question mark
      + shocked. Surprise is celebration + shock.
    </p>

    <figure
      id="emotions-semantic-interesting"
      class="fullscreen-diagram"
    ></figure>

    <p>
      We can also use semantic dictionaries to see how individual emotion
      neurons are used, like the mental health neuron we looked at earlier. We
      see the model sees stress as mental illness + success, anxiety as mental
      illness + confusion, and mad as mental illness + evil.
    </p>

    <figure id="emotions-semantic-mental" class="fullscreen-diagram"></figure>

    <p>
      We also see that not all emotions are clearly separated from other worldly
      objects. For example, part of powerful is a lightning neuron, part of
      creative is a painting neuron, part of embarrassed is a neuron
      corresponding to the decade 2000-2010, and part of let down is a neuron
      for destruction. Multiple senses of a word can also cause unexpected
      combinations. While content is composed of happiness neurons and
      relaxation neurons, it also contains a newspaper neuron.
    </p>

    <figure id="emotions-semantic-worldly" class="fullscreen-diagram"></figure>

    <p>
      We also see a number of examples of sensitive topics and bias being
      integrated into emotion vectors. For instance, accepted is composed of an
      LGBT neuron as well as neurons for cool and happiness. Others are more
      concerning. Confident is composed of a neuron for representing overweight
      humans + happy. Pressured is composed of a neuron for Asian culture as
      well as a graduation neuron. Humiliated is composed primarily from a
      neuron for Islam and Arabic text.
    </p>

    <figure id="emotions-semantic-bias" class="fullscreen-diagram"></figure>

    <p>
      In these experiments we’ve shown how attribution lets us to study points
      of emotion inaccessible by looking at one neuron at a time. But the
      emotions we’ve studied here only show a glimpse of the full space spanned
      by the emotion family, since they were simply the words on a single
      feeling wheel.
    </p>

    <p>
      We believe studying this space in more depth may be a good starting point
      to studying the spaces spanned by neuron families in general. Humans are
      extremely good at quickly processing emotion, and during our ad-hoc
      experiment using feature visualization on different combinations of
      neurons we were surprised how many we found intuitively legible. It may be
      the case that our perceptual systems are good enough to understand a large
      fraction of the emotion space in this model. If so, studying it may tell
      us more about the neurons of neuron families that work together, and
      provide insight into human emotions themselves.
    </p>

    <br />
    <hr />
    <br />

    <h2>Typographic Attacks</h2>

    <p>
      As we've shown, it's not just the "Language" side of the Contrastive Language/Image Pretrained Model
      that knows how to read: some features in the "Image" side appear to
      understand individual letters, numbers, and even full words.
    </p>

    <p>
      These features are clearly useful if the model needs perform OCR: characters and words can provide context cues for recognizing what an image is about.

      However, it's also worth noting that of all the ways the model could end up systematizing these typographic cues, it ends up clustering them into concepts. In the neurons of the last layer, text recognition comes bundled with recognition for other representations of a shared concept cluster, all recognized by a single neuron together. We can see this in the feature visualizations: the "kiss" neuron's feature visualization shows both the word "kiss" as well as actual kisses.
    </p>

    <figure id="literate-neurons"></figure>

    <p>
      This all should be no surprise by now: these are multimodal neurons, after all!
    </p>

    <p>
      Given how closely intertwined this text seems to be with the model's high-level representations, can the presence or absence of a piece of text
      influence the model's classifications?
    </p>

    <p>
      Surprisingly, we find that it can! Simply adding text to an image can
      cause the image to be classified as something it's not. This can be seen
      as a kind of adversarial attack – what we'll call a
      <i>typographic attack</i>.
    </p>

    <figure id="in-the-wild-1" class="fullscreen-diagram"></figure>

    <% let attacks = require('../static/typographic/in_the_wild_1.json') %>

    <p>
      Although we call these a kind of adversarial attack, we are mainly
      interested in what typographic attacks can tell us about the multimodal
      models, rather than seeing them as a competitive adversarial attack. After
      all, they're limited to models with multimodal neurons! Despite this, it
      is helpful to understand how these attacks fit within the broader
      tradition of adversarial attack research.
    </p>

    <p>
      While many adversarial attacks focus on making imperceptible changes to
      images, some attacks instead involve more exotic constraints. Typographic
      attacks are more similar to work in this second line of research,
      including <i>adversarial patches</i
      ><d-cite key="brown2017adversarialpatch"></d-cite> and
      <i>physical adversarial examples</i
      ><d-cite key="athalye2017adversarialturtle"></d-cite>. Adversarial patches
      are stickers that can be placed on a real-life object in order to cause
      neural nets to misclassify that object as something else – for example, a
      toaster. Physical adversarial examples are complete 3D objects that are
      reliabily misclassified: previous work gives a 3D-printed turtle that is
      reliably misclassified as a rifle and a baseball that is misclassified as
      an espresso.
    </p>

    <p>
      How robust are typographic attacks, and can they reliably cause a wide
      variety of images to become misclassified? Answering this using
      <a href="#in-the-wild-1">Figure [[N]]</a>'s physical setting unfortunately
      isn't scalable. To investigate typographic attacks more systematically,
      let's look at them in an automated setting.
    </p>

    <h3>Automating typographic attacks</h3>

    <p>
      We set up a simple automated attack. Each attack consists of an
      <i>attack text</i> that attempts to convert images to a
      <i>target class</i>. We place the text around the image at eight fixed
      coordinates<d-footnote
        >These coordinates aren't particularly special; we choose them just
        because they space the text out around the image.</d-footnote
      >
      and using a fixed font style.
    </p>

    <figure id="attack-setup" class="fullscreen-diagram"></figure>

    <p>
      We apply this attack to images from the ImageNet validation set, and test
      whether the attack is able to reliably switch a large percentage of
      ImageNet validation set images to the target attack class.
    </p>

    <p>We found text snippets for our attacks using a handful of techniques:</p>
    <ol>
      <li>
        Manually looking through the multimodal model's neurons for those that
        appear sensitive to particular kinds of text. This is how we found the
        <i>piggy bank</i> and <i>Siamese cat</i> attacks.
      </li>

      <li>
        Writing a genetic algorithm that modifies text in order to hill-climb
        towards text that is a more effective adversarial attack. This is how we
        found the <i>great white shark</i> and <i>waste container</i> attacks.
      </li>

      <li>
        Just brute-force searching through all of the ImageNet class names
        looking for class names which are, in and of themselves, effective
        attacks. This is how we found <i>rifle</i>, <i>pizza</i>, <i>radio</i>,
        <i>iPod</i>, and <i>library</i>.
      </li>
    </ol>

    <p>
      Under this attack setup, we found several attacks to be reasonably
      effective. Among the strongest of these attacks we see that this simple
      procedure can get us up to a 97% attack success rate with only around 7%
      of the image's pixels changed.
    </p>

    <figure id="automated-attacks" class="l-body-outset"></figure>

    <p>
      To contextualize how effective these results are as an adversarial attack,
      we can compare the linear probes results for the strongest two attacks
      above to the performance of the attacks described in
      <i>Adversarial Patch</i>. For reference, those attacks achieve up to 80%
      attack success at 8% pixel cover – many of the attacks above are as or
      more effective.
    </p>

    <p>
      We can improve upon the results from back in
      <a href="in-the-wild-1">Figure [[N]]</a> by constructing a new dataset of
      physical typographic attacks using some of these more effective attacks
      we've now discovered.
    </p>

    <figure id="in-the-wild-2" class="fullscreen-diagram"></figure>

    <!-- <h4>As a black-box attack</h4>
    <p>
      As a sidenote, one interesting aspect of these typographic attacks is that they may in some cases be effective as <b>few-shot, black-box adversarial attacks</b> – they could plausibly work even in settings where the attacker only has query access to the model.<d-cite
      key="ilyas2018blackbox"></d-cite>

      [[(Second reason, perhaps, but not sure if they are going to be THAT that effective)]] [[(Better to be explicitly humble so that the adversarial attack community knows that's what we're doing here)]]
    </p>

    <p>
      For models vulnerable to this category of attack, an adversary doesn't need to see the inside of the model in order to guess how to attack it: some common words that are likely enough to be related to the ImageNet class may suffice. What's more, the attack can be carried out with only a handful of queries to the model
    </p> -->

    <h3>Why do these attacks work?</h3>

    <p>
      We already know that the multimodal model develops high-level
      representations for concepts, and sometimes those concepts include both
      images and text. Naturally, some of these representations might be
      upstream of the model's representations of the various ImageNet classes.
    </p>

    <figure id="attackable-neurons"></figure>

    <p>
      Above, we see two examples of high-level representations correlated with
      ImageNet classes: a neuron responding to images of piggybanks, money, and
      prices, and a neuron responding to the Apple logo and images and names of
      Apple products.
    </p>

    <p>
      Because these high-level representations are interpretable, we can inspect
      the neurons that the model is using and their weights, and develop
      typographic attacks from there.
    </p>

    <p>
      Inasmuch as the model forms these high level representations, it's forming
      them because those representations <i>efficiently compress</i> the
      training set. And naturally, inputs like the ones in these typographic
      attacks – images spuriously labeled with irrelevant text – are
      <i>not</i> very common in that training set! As a result, a piece of text
      that might normally be a very strong cue may in an attack setting be
      strongly-weighted enough to get in the way of the model's perception of
      other objects elsewhere in the image, simply by outweighing them.
    </p>

    <p>
      The multimodal model's architecture is powerful enough to contain
      representations that learn both images and words, and one effect is what
      we see here: a unique weakness to typographic attacks.
    </p>

    <!-- <p>How else might we investigate just exactly how much text is integrated into the model's higher-level concepts [[edit: duplicative]]? One way we could lower-bound it is if we could show a downstream effect of [[that integration]]. -->

    <!-- In fact, we hypothesize that a fine-tuned model might actually <i>erase</i> this effect – by causing the model to no longer "need" to know how to read. The susceptibility to typographic attacks may well be an interesting consequence of the multimodal model being required to effectively perform at <i>diverse</i> tasks, not just at ImageNet as in these experiments but also at its native image captioning task. -->

    <!-- <p>[[TODO(Experiment+Analysis): Attempt to generate more nuanced analyses by subtracting activations.]]</p> -->

    <!-- <p>TODO(Writing): Gabe's point about the connection to fairness – overgeneralization. [[In the high-level representations. Connects to the model theory point.</p> -->

    <!-- <p>TODO(Writing): Connection to the Stroop effect?</p> -->

    <!--
      Still to-do once I have wifi:
      [] In the Wild 3 – sftp my images to athena.dialup.mit.edu, then run them
      [] Linear probes for In the Wild?? (Did I not have this working before? Huh? What was it that I'm missing here? Maybe I misremember the task?)
      [] Extract out a Svelte component for the image card
      [] Research the Stroop effect and get a couple of citations
      [] (Optional stretch) Extract out a Svelte component for In the Wild
      [] Rewrite the conclusion once I have more brain
    -->

    <br />
    <hr />
    <br />

    <h2>The Mechanics of Abstraction</h2>

    <p>
      One of the greatest mysteries of neural networks is how they can
      accomplish things which no human being knows how to directly write a
      computer program to do. This mystery is present in all non-trivial neural
      networks, but it burns especially brightly in light of a neural network
      which seems to have features previously thought to be signatures of the
      human mind.
    </p>
    <p>
      Recent work analyzing circuits [] has shed some light on the mechanism and
      algorithms running inside the layers of vision models. Building on a
      tradition of researchers studying meaningful neurons inside neural
      networks [], the circuits approach tries to systematically and rigorously
      characterize neurons inside neural networks, and then understand how the
      graph of weights between them implements their behavior. In this section,
      we aim to offer a very preliminary circuit analysis of mulltimodal models.
    </p>
    <div class="todo">
      <b>TODO(@colah)</b>: work in language about being preliminary
    </div>
    <p>
      The abstract features we saw in the previous section only form in the
      final layers of multimodal models. At a high-level, they seem to form by
      unioning over many cases, with every residual layer adding on new cases.
      For example, ...
    </p>

    <figure id="enrichment-diagram" class="fullscreen-diagram"></figure>
    <!--<div class='todo'><b>TODO(colah):</b></div>

    The abstract features we saw in the previous section are produced only after dozens of layers of visual processing -- what happened in those layers?
    -->

    <p>
      We call these circuits union over massive numbers of facets "enrichment
      circuits," and will return to them at the end of this section. But
      enrichment circuits and the abstract features they produce are only
      possible because all the layers up to that point built up a foundation of
      sophisticated features from which the abstract features can be assembled.
      And so if we truly want to understand abstract features, we need to start
      with these.
    </p>

    <h3>Universal Features and Circuits</h3>

    <p>
      As we examined the features and circuits in the early and mid layers of
      the multimodal models, we found many that appeared to be analogues of
      features found in other models. In particular, many of the features we
      found in early vision are strikingly similar to
      <a href="https://distill.pub/2020/circuits/early-vision/"
        >early visual features</a
      >
      of both ImageNet and Places365 models:
    </p>

    <figure id="early-vision" class="fullscreen-diagram"></figure>

    <p>
      Many of these features actually develop multiple times throughout the
      multimodal models we examined, at different scales. For example, we
      consistently see small black and white vs color detectors at the beginning
      of block 2, and then larger ones many layers later at the beginning of
      block 3. We also see interesting differences between the analogues.
    </p>

    <p>
      The idea that analogous features form across models is sometimes called
      the "<a>universality hypothesis</a>." A significant amount of work has
      measured the overall similarity of neural network representations, often
      finding significantly similarity between representations of different
      models []. A smaller amount of work has explored individual analogous
      features forming across models []. Previous demonstrations of universality
      have tended to show that different models trained on the same dataset
      develop similar features, leaving some uncertainty as to whether it would
      be true for models trained on very different datasets. The multimodal
      model provides an example of analogous features forming across models
      which are different in dataset, training objective, and architecture.
    </p>

    <p>
      Not only do multimodal models develop many features which have analogues
      InceptionV1, but we found a number of cases where those features seem to
      be implemented by analogous circuits:
    </p>

    <figure id="universal-circuit-diagram" class="fullscreen-diagram"></figure>
    <div class="placeholder">
      <b>TODO(@colah):</b> Figcaption explaining technical details
    </div>

    <p>
      Finding analogous circuits is a stronger result than finding analogous
      features. You could imagine finding highly-correlated and behaviorally
      similar features across two models, and then realizing they're implemented
      in different ways. (We see different implementations of the same function
      with different algorithms all the time in computer science: QuickSort and
      HeapSort implement the same sort function in different ways!) The fact
      that analogous features are implemented in the same way suggests that
      understanding the mechanisms underlying one model may also help us
      understand other models.
    </p>

    <p>
      We also see features which are analogues in more abstract ways. For
      example, InceptionV1 (ImageNet) has left-oriented and right-oriented head
      detectors which most strongly react to dog heads, which are then merged
      into orientation-invariant head detectors. Multimodal models also have
      left-oriented and right-oriented head detectors, but they most strongly
      react to human heads.<d-footnote
        >InceptionV1 (ImageNet) also has human-specific head detectors, but they
        don't seem to be oriented. Likewise, the multimodal models have
        dog-specific head detectors, but they don't appear to be
        oriented.</d-footnote
      >
      In both cases, the invariant head detectors are created by combining
      pose-specific head detectors.
    </p>

    <figure style="grid-column-start: text; grid-column-end: page">
      <img src="images/head-circuit.png" />
    </figure>

    <h3>Task-specific Features</h3>

    <p>
      In addition to "universal" features which the multimodal models seem to
      share with both ImageNet and Places365 models, we observed features which
      they seemed to share with only one:
    </p>

    <figure id="task-specific" class="fullscreen-diagram"></figure>

    <p>
      There are also features which don't have highly-correlated analogues in
      the ImageNet or Places365 model we studied, but still formed consistently
      across multimodal models. Many of these features seem like something that
      would form in another classification task! In fact, the palm tree
      detectors are very similar to those found in an unpublished visualization
      [] of the geolocalization model PlaNet[].
      <d-footnote
        >We know that the multimodal models can be used for geolocalization, and
        that regional neurons form in the late layers, so it makes sense to
        expect features useful for geolocalization to also exist in lower
        layers.</d-footnote
      >
      Similarly, one imagines that features like the "f"-detector could be found
      in OCR models.
    </p>
    <div class="todo"><b>TODO(@colah)</b>: add facial feature example?</div>

    <p>
      Perhaps the right way to understand mid-level vision in the multimodal is
      as a <em>union</em> over features from a very wide variety of tasks.
    </p>

    <div class="placeholder">
      <b>TODO(@colah, @ggoh)</b>: short section on typograpic features?
    </div>
    <div class="placeholder">
      <b>TODO(@colah, @ggoh)</b>: short section on facial features?
    </div>
    <div class="placeholder">
      <b>TODO(@colah)</b>: weird giant brightness detectors?
    </div>

    <h3>The mechanics of Abstraction</h3>

    <p>
      The middle layers of our models provide a variety of features: detectors
      for various kind of objects, for short words, and much more. But the final
      layers are where the most striking property of multimodal models occurs:
      the formation of abstract, multimodal features.
    </p>

    <p>
      We found circuits in these final layers particularly difficult to analyze.
      The layers have widths of several thousand neurons, depending on the
      model, and there appears to be quite a bit of polysemanticity. However,
      the residual architecture does give us one type of circuit for free: what
      we've called "encrichment circuits," where each residual block adds new
      facets to the abstract neurons.
    </p>

    <figure id="enrichment-diagram-2" class="fullscreen-diagram"></figure>
  </d-article>

  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>We are deeply grateful to... Imaginary Creatures</p>

    <p>Many of our diagrams are based on...</p>

    <h3>Author Contributions</h3>
    <p><b>Research:</b> Alex developed ...</p>

    <p><b>Writing & Diagrams:</b> The text was initially drafted by...</p>

    <h3 style="grid-row: auto / span 3">Appendix: Methodological Details</h3>

    <h4>Faceted Feature Visualization</h4>

    <p>
      A neuron is said to have multiple facets if there are multiple distinct
      cases that they fire for<d-cite key="nguyen2016multifaceted"></d-cite> --
      that is, if a feature can be described as <i>OR(case_1, case_2, …)</i>.
      For example, a pose-invariant dog-head detector that detects dog heads
      tilted to the left, right, or facing straight on<d-cite
        key="olah2020zoom"
      ></d-cite
      >, or a boundary detector which looks for a difference in texture from one
      side to the other but doesn’t care which is which<d-cite
        key="olah2020overview"
      ></d-cite
      >, or a “grocery store” output neuron that detects both exterior and
      interior views of a grocery store<d-cite
        key="nguyen2016multifaceted"
      ></d-cite
      >. (You may also be familiar with the idea of a “polysemantic”
      neuron<d-cite key="olah2017feature,olah2020zoom"></d-cite>. Polysemantic
      neurons are a special type of multifaceted neuron in which the facets are
      unrelated.)
    </p>

    <p>
      <a href="https://distill.pub/2017/feature-visualization/"
        >Feature visualization</a
      ><d-cite
        key="erhan2009visualizing,olah2017feature,simonyan2013deep,nguyen2015deep,mordvintsev2015inceptionism,nguyen2016plug"
      ></d-cite>
      is a technique where the input to a neural network is optimized to create
      a stimuli demonstrating some behavior, typically maximizing the activation
      of a neuron. There are two challenges with using feature visualization on
      multi-faceted neurons. Firstly, the optimization process may give us an
      out-of-distribution input that tries to activate multiple facets at once;
      this can sometimes help reveal the existence of multiple facets, but can
      also lead to difficult-to-parse or nonsensical stimuli. (We suspect this
      is true when the multiple facets aren’t mutually inhibiting; for contrast,
      see mutual inhibition in the InceptionV1 pose invariant dog head
      circuit<d-cite key="olah2020zoom"></d-cite>.) Secondly, even if the
      optimization process successfully reveals once facet, it may not reveal
      others.
    </p>

    <p>
      These challenges are of increased significance in the multimodal models we
      study in this paper, because the highly-abstract neurons at the end of the
      network seem to activate for an enormous variety of cases!
    </p>

    <p>
      We are aware of two past approaches to improving feature visualization for
      multi-faceted neurons. The first approach is to find highly diverse images
      which activate a given neuron, and use them as seeds for the feature
      visualization optimization process<d-cite
        key="nguyen2016multifaceted"
      ></d-cite
      >. This approach requires one to have access to the dataset, may taint the
      value of feature visualization in establishing evidence of causality, and
      the stimuli may still be pulled into a different basin of attraction. An
      alternative approach is to perform multiple feature visualizations at once
      optimizing for an additional “diversity term” which encourages the
      different feature visualizations to activate different lower-level
      neurons<d-cite key="olah2017feature"></d-cite>. However, this can incent
      the visualizations to include unrelated content to activate lower level
      neurons and increase the diversity term.
    </p>

    <p>
      We take a new variant of the second approach, adding a diversity term to
      the optimization process. However, we base our diversity term not on the
      <i>activation</i> of lower level neurons, but on the <i>attribution</i> of
      the high-level neuron to lower level neurons. The intent here is to only
      incent variation in low-level neurons if it affects the neuron we’re
      visualizing. To formalize this, we use a simple
      <i>attribution = gradient ⊙ activation</i> attribution method, which can
      be seen as the linear approximation of the effect of the lower level
      neurons on the high level one. Since this is a resnet where there is a
      linear pathway between them, this seems especially principled. We then
      maximize the orthogonal components of these attribution vectors . A
      reference implementation can be found in attached colab notebooks.
    </p>

    <p>
      Sometimes it’s desirable to visualize analogous types of facets across
      many neurons in a controlled manner. In the multimodal models, many
      neurons have a “text” facet -- in fact, many neurons have lots of
      different text facets, responding to many words! -- and we’d like to see
      them for all neurons. To do this, we first collect images of each type
      (text, faces, etc) and fit a logistic regression. We then add a targeted
      diversity term <i>regression weights ⊙ attribution</i> to encourage a
      facet that activates our high-level neuron through low-level neurons
      associated with a given facet type. Again, a reference implementation can
      be found in attached colab notebooks.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
