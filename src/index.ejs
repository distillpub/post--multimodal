<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>

  <style>
    .fullscreen-diagram{
      grid-column: screen; 
      padding:0px; 
    }

    .fullscreen-table{
      margin-top:0px;
      margin-bottom:0px; 
      grid-column: screen; 
      background: #FFFFF; 
    }

  </style>

</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Multimodal Representation in the Multimodal Model</h1>
  </d-title>

  <d-article>

    <p>
      Learning representations that match those in the human brain is a holy grail of machine learning. Two parallel shifts in the field give us hope that our models are moving closer to this goal. 
    </p>
    <p>
      First, hand-crafted datasets are giving way to data scraped and sampled from epicenters of human activity like Reddit and Twitter. This data is chaotic, unstructured, resembling the world people interact with during their development, a far cry from the highly structured hand-labeled datasets of old.
    </p>
    <p>
      Secondly, the human brain needs to work with several modalities such as vision and scent to perceive the world around them. Several modern neuroscience theories propose that different brain regions, such as the auditory cortex and visual cortex each process low level inputs into a shared abstract representation that can be fed to a global workspace for further processing. Recent work in multimodal neural networks, and in particular those trained with contrastive loss share a similar architecture, with parallel streams converting different modalities like text and images into a shared representation.
    </p>
    <p>
      To see whether our hope was warranted we looked inside the multimodel model presented in Radford et al to study its representations. We find a landscape of neurons for cultural figures like Donald Trump and Justin Bieber, with neurons corresponding to geographical regions and cultures across the world, and families of neurons with a rich understanding of human emotions. Interestingly, every one of these neurons are what we're calling multimodal neurons, closely mimicking the famed "Halle Berre neuron" found in the human visual cortex that responds to both pictures of the address as well as pictures of her name.
    </p>

    <p>
      To see whether our hope was warranted we looked inside the multimodels presented in Radford et al to understand its representations. We found vast landscapes of neurons for cultural figures like Donald Trump and Justin Bieber, neurons corresponding to geographical regions and cultures across the world, and families of neurons with a rich understanding of human emotions. Excitingly, every one of these neurons are multimodal, sometimes unifying up to six modalities, closely resembling the famous "grandmother neuron" result from neuroscience.
    </p>

    <p>
      Here we investigate the model presented in Radford [] ... where a battery of quantitative evaluations revealed that a simple dot product, with the right query, can produce meaningful answers to questions that span the pantheon of human meaningful tasks - optical character recognition, geolocation, facial recognition, imagenet object identification, and even basic visual question answering. How does a simple vector achieve such feats?
    </p>

    <p>
      Amongst the highlights of our discoveries, we discover neurons that bear an uncanny resemblance to those representations found in the brain -- multimodal neurons. Much like the famed “Halle Berry” or “Catwomen” biological neurons, we too find neurons that exhibit invariance and selectivity to famous identities, such as:
    </p>
  
    <p>    
      [Images of Spiderman] + [Sketches of Spiderman] + [the word “spiderman”]
      [Photos of Obama/Trump] + [Political sketches of Obama/Trump] + [the words “president”, obama, trump]
    </p>

    <p>
      It is tempting to declare that we have found examples of grandmother neurons in our representations, but our investigations reveal much more than meets the eye. For instance, we discover the Obama neuron also responds to an image with the text “President”, and Spiderman neuron activates both to Spiderman, his nemesis Venom, and his secret identity, Peter Parker. 
    </p>

    <p>
      Indeed, in the examples above, we find that the neurons respond to a web of associations centered around an abstract concept. In fact, a majority of these neurons appear to follow such a pattern, such as
    </p>

    <p>
      [Parenthood] [Christmas] [etc]

      Features of this kind cannot be pinned down to an artifact of a specific hyperparameter configuration. Our investigation spans a range of multimodal prototypes -- these models contain not only architectural variations with changes in hyperparameters, but are trained according to different objectives, including an autoregressive objective, as well as a contrastive objective. In fact, these models are even trained on different <i>data</i> - with some models trained on only Twitter data, and others a combination of Twitter and Reddit, and some on the full dataset mix - a mix of flickr, reddit, twitter data. 
    </p>

    <p>
      Yet despite the differences in modeling decisions, most of these models converge on very similar representations and all adhere to similar modes of organization. We are convinced therefore, that these qualitative properties we investigate are not artifacts of training, but are critical properties of a high quality representation [footnote].
    </p>
  
    <section>Into the Multimodal Mind</section>

    <h3>Identity Recognition</h3>

    <p>
      Our discussion begins with perhaps the kind of neuron that has fueled the most speculation in the brain, “grandmother” neurons. 
    </p>
    
    <p>
      A Donald Trump neuron. A Barack Obama neuron. A Queen Elizabeth neuron. Justin Bieber. Margaret Thatcher. Pope Francis. Spiderman. Jesus Christ. Adolf Hitler. Elvis Presley. 
    </p>

    <p>
      As discussed in the introduction, these person-detecting neurons are perhaps the most striking aspect of the models we studied.
    </p>

    <p>
      Like similar units found in neuroscience, these person detectors are multimodal. Not only do they respond to images of their subject, but they respond to images of their name, and also to drawings, cartoons, caricatures, and effigies. In fact, features visualizations of these neurons -- the result of starting with an image full of random noise and optimizing it to cause the neuron to fire -- often produce both the subject's face and name:
    </p>

    <p>
      [Feature visualizations, cherry picked for both face and name]
    </p>
    
    <p>
      We find a significant number of these person-detecting neurons. Every model we've studied has a Donald Trump neuron. We also found Barack Obama, Queen Elizabeth, and Justin Bieber neurons in 4 of 6 models. In contrast, Elvis Presley was only found in one model.  It seems likely this reflects the volume of discourse about these figures on Twitter at the time the data was collected in [2019? 2020].
    </p>

    <p>
      [Table of people-detecting neurons across models, based on Gabe's spreadsheet. Perhaps subject as row, model as col, and face-facet feature vis as entry in table when found. Perhaps include count and sort by number of occurrences of figure?]
    </p>

    <p>
      Person detecting neurons tend to be very selective for their subject when they activate at peak magnitude. But on closer investigation, that isn't the complete story. They tend to also fire, albeit more weakly, for other people and content related to their subject. For example, Donald Trump neurons generally also fire for Mike Pence, while Barack Obama neurons also respond to Michelle Obama (and Joe Biden?), and Spiderman neurons also respond to related characters like Venom and Black Panther.
    </p>

    <p>
      [Figure: Conditional probabilities of different stimuli types conditioned on activation strength of neuron]
    </p>

    <p>
      This is similar to how [neuroscience analogy]
    </p>
    
    <p>
      Since these units also respond to other people, thinking of them as people detectors may not actually be the best conceptualization. Rather, it might be more helpful to think of them as topic detectors, with the person as the foremost symbol of that topic. Or to think of them as associative people detectors, firing based on how associated content is to their subject. [Work the Bill Gates/Mike Zukerburg stuff into here.]
    </p>

    <h3>Geolocation</h3>
    
    <p>
      We also find "regional neurons" which respond to a fusion of features roughly associated with a geographic region: country and city names, distinctive architecture, prominent public figures, faces of the most common ethnicity, distinctive clothing, wildlife, and local script (if not roman alphabet). If shown a world map, even without labels, these neurons fire selectively for the relevant region on the map.
    </p>

    <p>
      Most often, these correspond to a continent (e.g. Africa, Australia, Europe), clusters of countries (e.g. Islamic Countries, India/Pakistan, East Asia, Latin America), or individual countries (e.g. USA, UK, China, Japan). We also observe neurons for larger geographical features, such as a northern hemisphere neuron (which responds to bears, moose, coniferous forest, and the entire northern hemisphere on a map), and a "tropical" neuron for regions along the equator.
    </p>

    <p>     
      [Figure: similar to NMF diagram in building blocks, associate neurons with colors, then highlight region on world map with color, and also show faceted feature visualization for each neuron. Possibly show dataset examples as well]
    </p>

    <p>
      Again, many neurons of the same neurons exist across models. In particular, Africa, Australia, United States, and China neurons seem to form very consistently. [Table of region-detecting neurons across models. Perhaps subject as row, model as col, and face-facet feature vis as entry in table when found. Perhaps include count and sort by number of occurrences of figure?]
    </p>

    <p>
      In addition to these regional neurons, we find that many other neurons appear to have geographic information baked in, firing weakly for regions on a world map related to them. For example, we noticed that a coffee neuron fires for Brazil, where a significant amount of coffee is made, and a lion/tiger neuron fires for the parts of Africa and Asia where lions and tigers are found, a common human mistake! We're hesitant to read too much into this -- there's plenty of neurons which activate a bit for a world map without any obvious relationship to the topic -- but it is an interesting phenomenon.
    </p>

    <h3>Table of Interesting Neurons</h3>

    <p>
      These abstractions appear to span the extremely low level - the nearly raw representation of color, characters, to the extremely abstract - neurons that represent all mammals, entire countries and ecosystems. Why do the representations form in this way? 
    </p>

    <figure id="interesting-neurons" class="fullscreen-table"></figure>

    <p>
      These abstractions appear to span the extremely low level - the nearly raw representation of color, characters, to the extremely abstract - neurons that represent all mammals, entire countries and ecosystems. Why do the representations form in this way? 
    </p>

    <h2>Mechanics of Abstraction</h2>

    <p>
      These abstractions appear to span the extremely low level - the nearly raw representation of color, characters, to the extremely abstract - neurons that represent all mammals, entire countries and ecosystems. Why do the representations form in this way? 
    </p>

    <p>
      One clue, observed by Radford [], is that these representations of this form are useful. We observe, in fact, that even extremely sparse combinations of these neurons can combine to solve an uncanny number of tasks considered human-meaningful.
    </p>

    <p>
      The IILSRV challenge, for example, uses a subset of the wordnet hierarchy, is an explicit attempt to systematize human knowledge. The task involves making fine-grained decisions about the subject of a picture. 
    </p>

    <p>
      To understand how the neurons compose themselves to a meaningful task, we follow the methodology of Radford [] et all, and train a linear probe logistic regression classifier on imagenet to tease out which features are most relevant to the problem of classification. with only 3, on average, nonzeros per class, to understand how neurons themselves can assist in this task.
    </p>

    <p>
      As the above article might predict, like [], it should come as no surprise to the reader that much of the information in the model is consolidated in single neurons. To our surprise, however, we find much more interesting additional structure in this information. The neurons, for example, arrange themselves into a taxonomy of classes that appear to respect the wordnet hierarchy. 
    </p>

    <p>
      At the highest leve we find a single neuron that represents the split between the living - animal, and the nonliving, that fires for nearly all the animals in the 1000 classes chosen.
    </p>

    <p>
      The animal kingdom itself is split into the domesticated pets and wildlife. 
    </p>
    
    <p>
      More conventionally, the animal kingdom is also split into more conventional categories, such as insects, birds and reptiles.
    </p>

    <p>
      We see other classes too which do not correspond neatly to such classes organized by experts, but nevertheless make sense. We see, for example, three neurons that respond to creatures found in different aspects of the ocean/water.
    </p>

    <p>      
      These classifications are not limited, in fact, to animals. Vehicles, too, have their own implicit hierarchy, here with ...
    </p>

    <figure id="hypergraph-device" class="fullscreen-diagram"></figure>

    <p>
      We find this remarkable given the fact that such organization is not, in any explicit form, given as a training signal to the neural network. The neural network has decided that the most efficient way to organize information is in this way, one which reflects human intuition. Perhaps this form of convergent evolution is a suggestion that these structures do exist in some implicit form in human language, and though there may be dispute as to where the lines are drawn explicitly, 
    </p>

    <p>
      We make a final note of a few classes in imagenet that do not fall nearly into one of the above large hierarchies. The “piggy bank” class in imagenet, for example, appears to be a singularity, but a reasonable accuracy on the class can still be obtained by combining neurons that respond to abstract concepts, e.g.
    </p>


    <h2> Understanding language </h2>

    <p>
      The linear probes are the most straightforward way to understand the neuron’s uses, but they merely index into a small number of classes. To understand the model’s capacity for understanding language, we need tools that allow us to understand an exponential number of classes - one for every possible combination of tokens in the language model.
    </p>

    <p>
      The contrastive loss of the transformer takes sentences into embeddings. Thus, the language model has done much of the difficult work for us, and we only need to understand, if given a set of sentences, the continuous space in which the sentences are embedded. 
    </p>

    <subsection>Using Abstractions</subsection>
    
    <h2>The Mechanics of Abstraction</h2>

    <p>
      One of the greatest mysteries of neural networks is how they can accomplish things which no human being knows how to directly write a computer program to do. This mystery is present in all non-trivial neural networks, but it burns especially brightly in light of a neural network which seems to have features previously thought to be signatures of the human mind. Concretely, the abstract features we saw in the previous section are produced only after dozens of layers of visual processing -- what happened in those layers?
    </p>
    <p>
      Recent work analyzing circuits [] has shed some light on the mechanism and algorithms running inside the layers of vision models. Building on a tradition of researchers studying meaningful neurons inside neural networks [], the circuits approach tries to systematically and rigorously characterize neurons inside neural networks, and then understand how the graph of weights between them implements their behavior.
    </p>
    <p>
      As we examined the features and circuits in the hidden layers of the multimodal models, we found many that appeared to be analogues of features we'd discovered in ImageNet [] models like InceptionV1 []. But it wasn't just ImageNet models. We also saw features -- such as palm tree detectors -- we'd seen in an unpublished visualization [] of the geolocalization model PlaNet []. Others seemed familiar from looking at InceptionV1 trained on Places365 []. And while we'd never seen visualizations of a strong OCR model, we discovered many lexicographic features that would intuitively form in such a model.
    </p>

    <figure id="universality-diagram" class="fullscreen-diagram"></figure>

    <section>Universiality</section>

    <section>Multimodal Circuits</section>

    <section>Typographic Attacks</section>

  </d-article>

  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to... Imaginary Creatures
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>