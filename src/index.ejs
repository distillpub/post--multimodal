<!DOCTYPE html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Multimodal Representation in the Multimodal Model</h1>
  </d-title>

  <d-article>
   

    <d-contents>
      <nav class="toc figcaption">
        <h4>Contents</h4>
        <!--<div><a href="#introduction">Introduction</a></div>-->
        <div><a href="#">Introduction</a></div>
        <div><a href="#">Into the Multimodal Mind</a></div>
        <ul>
          <li><a href="#">Identity Recognition</a></li>
          <li><a href="#">Regional Neurons</a></li>
          <li><a href="#">Emotion Neurons</a></li>
          <li><a href="#">Miscellaneous Neurons</a></li>
        </ul>
        <div><a href="#">Using Abstractions</a></div>
        <ul>
          <li><a href="#">Understanding language</a></li>
          <li><a href="#">Emotional Intelligence</a></li>
          <li><a href="#">Miscellaneous Neurons</a></li>
        </ul>
        <div><a href="#">Typographic Attacks</a></div>
        <div><a href="#">The Mechanics of Abstraction</a></div>
        <ul>
          <li><a href="#">Universal Features and Circuits</a></li>
          <li><a href="#">Task-specific Features</a></li>
          <li><a href="#">The mechanics of Abstraction</a></li>
        </ul>
        <div><a href="#">Conclusion</a></div>
        <div><a href="#">Appendix: Methodological Details</a></div>
      </nav>
      <div class='toc-line'></div>
    </d-contents>

  <p>In 2005, a letter published in Nature described human neurons responding primarily to specific people, such as Jennifer Anniston or Halle Berry, as well as landmarks <d-cite key="quiroga2005invariant"></d-cite>. The exciting thing wasn’t just that they selected for particular people,
    <d-footnote>previous results had reported units that respond to particular faces []</d-footnote>
  but that they did so regardless of whether they were shown images, drawings, or even images of the person’s name. The neurons were <i>multimodal</i>. As the lead author would put it: "You are looking at the far end of the transformation from metric, visual shapes to conceptual… information."
    <d-footnote>Quiroga's full quote, from <a href='https://www.newscientist.com/article/dn7567-why-your-brain-has-a-jennifer-aniston-cell/'>New Scieintist</a> reads: "I think that’s the excitement to these results. You are looking at the far end of the transformation from metric, visual shapes to conceptual memory-related information. It is that transformation that underlies our ability to understand the world. It’s not enough to see something familiar and match it. It’s the fact that you plug visual information into the rich tapestry of memory that brings it to life." We elided the portion discussing memory since it was less relevant.</d-footnote></p>

  <p>We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, 
  such as Donald Trump, 
  Lady Gaga, and Spiderman.
    <d-footnote>It’s important to note that the vast majority of people these models recognize don’t have a specific neuron, but instead are represented by a combination of neurons. Often, the contributing neurons are conceptually related. For example, the Trump neuron fires (albeit more weakly) for Mike Pence, contributing to representing him.</d-footnote>
    <sup class='footnote-sep'></sup>
    <d-footnote>Some of the figures we found neurons for are divisive. It should go without saying that having a dedicated neuron simply reflects the prominence of these figures in the training data, collected in [2019?]. Being divisive increases this prominence, because divisive figures are more likely to be extensively discussed on the Internet. In the case of the Donald Trump neuron, it seems likely there would have also been a Hillary Clinton neuron if data had been collected in 2016 instead. (There are other neurons which respond to Hillary Clinton in addition to other topics.) </d-footnote>
    <sup class='footnote-sep'></sup>
    <d-footnote>Not only do our neurons match Quiroga et al in firing for the same feature in photo, drawing, and text form, but some of the specific neurons are strikingly similar to those they describe. The Donald Trump neuron we found might be seen as similar to Quiroga et al’s Bill Clinton neuron. A Star Wars neuron we find seems analogous to a biological Star Wars neuron described Quiroga et al’s follow up paper []. And although we don’t find an exact Jennifer Anniston neuron, we do find a neuron for the TV show “Friends” which fires for her.</d-footnote>
  Like the biological multimodal neurons, these neurons respond to the same subject in photographs, drawings, and images of text:
</p>

<figure style="display: grid; grid-column-gap: 64px; grid-row-gap: 16px; grid-template-columns: repeat(2, auto); max-width: 600px;">
  <div class='figcaption'>Just as the biological Halle Berry neuron responds to photos, drawings, and text...</div>
  <div class='figcaption'>... multimodal neurons in artificial neural networks activate for photos, cartoons, and text as well.</div>
  <img src="images/IntroHalleBerry.png" />
  <img src="images/IntroTrump.png" />
</figure>

<div class='todo'><b>TODO:</b> Trump neuron should be replaced or supplemented here once we have more dataset examples and find drawing/cartoon examples for other neurons.</div>

<p>
  But these people-detecting neurons only scratch the surface of highly abstract neurons we found. We also find features coding for emotions (eg. happy, sad, surprised), for regions of the world (eg. Northern Hemisphere, China, Philadelphia), for general physical features (eg. age, gender, mustache), for abstract topics (eg. family, Christmas, science), for fictional settings (eg. Star Wars, Avengers, Harry Potter), for styles (eg. child’s drawing, painting), and much more. 
 These neurons generalize in highly abstract ways beyond Quiroga et al’s photo/drawing/text trifecta. For example, the regional neurons respond to the relevant regions on a world map, to distinct local wildlife, unusual products, apparel, architecture, politicians, dominant ethnicities, local script, landmarks, the names of countries and cities, and much more.
</p>


<figure id="feature-overview" class='fullscreen-diagram'>
</figure>


<p>
We find these neurons in multimodal models [final name] from Radford et al [], although it’s possible similar features exist in earlier multimodal models. These models are trained to align pairs of images and text from the internet with their corresponding match, using a contrastive loss. 
<d-footnote>The authors also kindly shared an alternative version from earlier experiments, where the training objective was an autoregressive language modelling cost, instead of a contrastive objective. The features seem pretty similar.</d-footnote>
The visual side is a residual network [] of varying sizes. We refer readers to their paper for a detailed description.
</p>

<p>
In some ways, it seems natural for multimodal models to form these abstract visual features. We expect word embeddings and language models to learn abstract, topical features []. Arguably, what we’re seeing is the vision model learning to align with the native abstraction of language models.
<d-footnote>Many researchers are interested in “grounding” language models by training them on tasks involving another domain, in the hope of them learning a more real world understanding of language. The abstract features we find in vision models can be seen as a kind of “inverse grounding”: vision taking on more abstract features by connection to language.</d-footnote>
<!--Should add some kind of discussion to section 1  if we want to include the following:
<d-footnote>This includes some of the classic kinds of bias we see in word embeddings: for example, a feature that responds to rastered images of the following words: housewives, kathy, britney, jennifer, elizabeth, amanda, madonna, daughters, pregnancy, pharmaceutical, pharmaceuticals, amy, tiffany, barbara, daughter, healthcare.</d-footnote>-->
</p>

<p>
We begin this article with a <a>preliminary analysis</a> of some of the features that exist in these multimodal models. From there, we’ll explore <a>how those features are used</a> to accomplish the models goal of aligning images and text. This leads us to discover that these models are vulnerable to a kind of <a>“typographic attack”</a> where adding the right text to images can cause them to be systemically misclassified.
</p>

<figure id='intro-attack-demo'>
</figure>

<p>Finally, we’ll explore some of the intermediary features and circuits that build up abstract features. These explorations will require a few novel methods which we discuss in detail in an appendix 1.</p>



    <br />
    <hr />
    <br />

    <h2>Into the Multimodal Mind</h2>

    <h3>Identity Recognition</h3>

    <p>
      Our discussion begins with perhaps the kind of neuron that has fueled the
      most speculation in the brain, “grandmother” neurons.
    </p>

    <p>
      A Donald Trump neuron. A Barack Obama neuron. A Queen Elizabeth neuron.
      Justin Bieber. Margaret Thatcher. Pope Francis. Spiderman. Jesus Christ.
      Adolf Hitler. Elvis Presley.
    </p>

    <p>
      As discussed in the introduction, these person-detecting neurons are
      perhaps the most striking aspect of the models we studied.
    </p>

    <p>
      Like similar units found in neuroscience, these person detectors are
      multimodal. Not only do they respond to images of their subject, but they
      respond to images of their name, and also to drawings, cartoons,
      caricatures, and effigies. In fact, features visualizations of these
      neurons -- the result of starting with an image full of random noise and
      optimizing it to cause the neuron to fire -- often produce both the
      subject's face and name:
    </p>

    <p>[Feature visualizations, cherry picked for both face and name]</p>

    <p>
      We find a significant number of these person-detecting neurons. Every
      model we've studied has a Donald Trump neuron. We also found Barack Obama,
      Queen Elizabeth, and Justin Bieber neurons in 4 of 6 models. In contrast,
      Elvis Presley was only found in one model. It seems likely this reflects
      the volume of discourse about these figures on Twitter at the time the
      data was collected in [2019? 2020].
    </p>

    <p>
      [Table of people-detecting neurons across models, based on Gabe's
      spreadsheet. Perhaps subject as row, model as col, and face-facet feature
      vis as entry in table when found. Perhaps include count and sort by number
      of occurrences of figure?]
    </p>

    <p>
      Person detecting neurons tend to be very selective for their subject when
      they activate at peak magnitude. But on closer investigation, that isn't
      the complete story. They tend to also fire, albeit more weakly, for other
      people and content related to their subject. For example, Donald Trump
      neurons generally also fire for Mike Pence, while Barack Obama neurons
      also respond to Michelle Obama (and Joe Biden?), and Spiderman neurons
      also respond to related characters like Venom and Black Panther.
    </p>

    <p>
      [Figure: Conditional probabilities of different stimuli types conditioned
      on activation strength of neuron]
    </p>

    <p>This is similar to how [neuroscience analogy]</p>

    <p>
      Since these units also respond to other people, thinking of them as people
      detectors may not actually be the best conceptualization. Rather, it might
      be more helpful to think of them as topic detectors, with the person as
      the foremost symbol of that topic. Or to think of them as associative
      people detectors, firing based on how associated content is to their
      subject. [Work the Bill Gates/Mike Zukerburg stuff into here.]
    </p>

    <p>
      We also find "regional neurons" which respond to a fusion of features
      roughly associated with a geographic region: country and city names,
      distinctive architecture, prominent public figures, faces of the most
      common ethnicity, distinctive clothing, wildlife, and local script (if not
      roman alphabet). If shown a world map, even without labels, these neurons
      fire selectively for the relevant region on the map.
    </p>

    <p>
      Most often, these correspond to a continent (e.g. Africa, Australia,
      Europe), clusters of countries (e.g. Islamic Countries, India/Pakistan,
      East Asia, Latin America), or individual countries (e.g. USA, UK, China,
      Japan). We also observe neurons for larger geographical features, such as
      a northern hemisphere neuron (which responds to bears, moose, coniferous
      forest, and the entire northern hemisphere on a map), and a "tropical"
      neuron for regions along the equator.
    </p>

    <figure id="regional-neurons" class="fullscreen-diagram"></figure>
    <p>
      [Figure: similar to NMF diagram in building blocks, associate neurons with
      colors, then highlight region on world map with color, and also show
      faceted feature visualization for each neuron. Possibly show dataset
      examples as well]
    </p>

    <p>
      Again, many neurons of the same neurons exist across models. In
      particular, Africa, Australia, United States, and China neurons seem to
      form very consistently. [Table of region-detecting neurons across models.
      Perhaps subject as row, model as col, and face-facet feature vis as entry
      in table when found. Perhaps include count and sort by number of
      occurrences of figure?]
    </p>

    <p>
      In addition to these regional neurons, we find that many other neurons
      appear to have geographic information baked in, firing weakly for regions
      on a world map related to them. For example, we noticed that a coffee
      neuron fires for Brazil, where a significant amount of coffee is made, and
      a lion/tiger neuron fires for the parts of Africa and Asia where lions and
      tigers are found, a common human mistake! We're hesitant to read too much
      into this -- there's plenty of neurons which activate a bit for a world
      map without any obvious relationship to the topic -- but it is an
      interesting phenomenon.
    </p>

    <p>
      The task of captioning Internet images requires emotional intelligence, as
      small changes in a person's expression can radically change the meaning of
      a photo. We've found dozens of emotion detecting neurons in the multimodal
      model, each responding to the concept of a single emotion across a wide
      range of modalities, from body language and facial expressions, to text,
      to different species and cartoons, and even landscapes and the insides of
      buildings that evoke a feeling. There are neurons that respond to common
      feelings like anger and happiness, and as well as more nuanced emotions
      like feeling destroyed, aroused, honored, or silly.
    </p>
    <figure id="emotions-intro" class="fullscreen-diagram"></figure>
    <p>
      These emotions neurons are versatile and highly multimodal, spanning
      facial expressions across age, cartoon characters, relevant text, and even
      species. The surprise neuron activates even when the majority of the face
      is obscured. It responds to slang like "OMG!" and "WTF", and text feature
      visualization produces similar words of shock and surprise.
    </p>
    <figure id="emotions-surprise" class="fullscreen-diagram"></figure>
    <p>
      Other neurons learn to detect emotions as part of a broader concept. At
      first glance neuron xyz responds to the concept of sexuality, activating
      in response to visual pornographic content, sexual slang, and the names of
      adult websites. However, using a facial facet with feature visualization,
      we see it also corresponds to the emotion of arousal, rendering a balding
      male with an exaggerated aroused expression. Similarly, we find a question
      mark neuron hides … In each of these cases, the neuron indeed detects an
      emotion, but since it is only part of a more abstract concept, we need
      facets to see it.
    </p>
    <figure id="emotions-minor" class="fullscreen-diagram"></figure>
    <p>
      On the other extreme, some neurons respond simply to specific body and
      facial expressions, like the silly expression neuron. It activates most to
      the internet-born duckface expression, and peace signs, and both words
      show up in its text feature visualization.
    </p>
    <figure id="emotions-duckface" class="fullscreen-diagram"></figure>
    <p>
      We're excited at the promise of emotion neurons to benefit social studies
      that need algorithmic access to emotion detectors. These neurons could
      help people understand the emotional content of a long video, either by
      coloring the video's scrubber based on the emotions at each time, or by
      showing an Activation Atlas of the scenes of a movie based on emotion to
      study cinematography in a new way. These emotions could also be useful for
      studying how expression changes over time. With a dataset of selfies over
      the last decade with their location tagged, perhaps one could better
      understand birth and spread of expressions like the duckface across
      different cultures and geography.
    </p>
    <h4>Mental Health Neuron</h4>
    <p>
      One neuron that doesn't represent a single emotion but rather a high level
      category of emotions is the mental illness neuron, which is separate from
      a different physical illness neuron. It responds to both body language,
      facial expressions, and text corresponding to a variety of emotions like
      anxiety, depression, and loneliness. It also responds to images and text
      of illegal drugs and medicine, and psychological words like "mental" and
      "psychology".
    </p>

    <p>
      To better understand the behavior of this neuron we collected more than
      300 dataset examples at a variety of activations and labelled them by hand
      into categories the neuron seemed to respond to. During the labeling
      process we did not have access to the activation. We can use this
      hand-labelled data to estimate the conditional probability of a category
      at a given activation.
    </p>

    <p>
      We see that the neuron activates most strongly to mental health disorders,
      but also to the broader concept of psychology. We can also look at the
      pre-ReLU activations to see what the neuron negatively responds to. It has
      a weak negative response to concepts we may expect to be negatively
      correlated with mental health disorders, like pets, sports, and travel.
      Many of its strongest negative activations are to exercise, sporting
      events, and music related ideas like albums and concerts.
    </p>
    <figure id="emotions-mentalhealth" class="fullscreen-diagram"></figure>


    <h3>Miscellaneous Neurons</h3>

    <figure id="literate-neurons"></figure>


    <br />
    <hr />
    <br />

    <h2>Using Abstractions</h2>

    <p>
      These abstractions appear to span the extremely low level - the nearly raw
      representation of color, characters, to the extremely abstract - neurons
      that represent all mammals, entire countries and ecosystems. Why do the
      representations form in this way?
    </p>

    <p>
      One clue, observed by Radford [], is that these representations of this
      form are useful. We observe, in fact, that even extremely sparse
      combinations of these neurons can combine to solve an uncanny number of
      tasks considered human-meaningful.
    </p>

    <p>
      The IILSRV challenge, for example, uses a subset of the wordnet hierarchy,
      is an explicit attempt to systematize human knowledge. The task involves
      making fine-grained decisions about the subject of a picture.
    </p>

    <p>
      To understand how the neurons compose themselves to a meaningful task, we
      follow the methodology of Radford [] et all, and train a linear probe
      logistic regression classifier on imagenet to tease out which features are
      most relevant to the problem of classification. with only 3, on average,
      nonzeros per class, to understand how neurons themselves can assist in
      this task.
    </p>

    <p>
      As the above article might predict, like [], it should come as no surprise
      to the reader that much of the information in the model is consolidated in
      single neurons. To our surprise, however, we find much more interesting
      additional structure in this information. The neurons, for example,
      arrange themselves into a taxonomy of classes that appear to respect the
      wordnet hierarchy.
    </p>

    <p>
      At the highest leve we find a single neuron that represents the split
      between the living - animal, and the nonliving, that fires for nearly all
      the animals in the 1000 classes chosen.
    </p>

    <p>
      The animal kingdom itself is split into the domesticated pets and
      wildlife.
    </p>

    <p>
      More conventionally, the animal kingdom is also split into more
      conventional categories, such as insects, birds and reptiles.
    </p>

    <p>
      We see other classes too which do not correspond neatly to such classes
      organized by experts, but nevertheless make sense. We see, for example,
      three neurons that respond to creatures found in different aspects of the
      ocean/water.
    </p>

    <p>
      These classifications are not limited, in fact, to animals. Vehicles, too,
      have their own implicit hierarchy, here with ...
    </p>

    <figure id="hypergraph-device" class="fullscreen-diagram"></figure>

    <p>
      We find this remarkable given the fact that such organization is not, in
      any explicit form, given as a training signal to the neural network. The
      neural network has decided that the most efficient way to organize
      information is in this way, one which reflects human intuition. Perhaps
      this form of convergent evolution is a suggestion that these structures do
      exist in some implicit form in human language, and though there may be
      dispute as to where the lines are drawn explicitly,
    </p>

    <p>
      We make a final note of a few classes in imagenet that do not fall nearly
      into one of the above large hierarchies. The “piggy bank” class in
      imagenet, for example, appears to be a singularity, but a reasonable
      accuracy on the class can still be obtained by combining neurons that
      respond to abstract concepts, e.g.
    </p>

    <h3>Understanding language</h3>

    <p>
      The linear probes are the most straightforward way to understand the
      neuron’s uses, but they merely index into a small number of classes. To
      understand the model’s capacity for understanding language, we need tools
      that allow us to understand an exponential number of classes - one for
      every possible combination of tokens in the language model.
    </p>

    <p>
      The contrastive loss of the transformer takes sentences into embeddings.
      Thus, the language model has done much of the difficult work for us, and
      we only need to understand, if given a set of sentences, the continuous
      space in which the sentences are embedded.
    </p>

    <h3>Emotional Intelligence</h3>

    <p>
      Earlier we looked at a family of neurons that each detect an emotion via
      several facets like text as well as facial and body expressions. These
      neurons only scratch the surface of the range of emotions the model
      understands, since most emotions are the result of several neurons working
      in concert, often alongside non-emotion neurons that are specific to each
      picture. In this section well study this broader landscape of emotions.
    </p>
    <p>
      One simple way to get the set of neurons that corresponds to an emotion is
      to take attribution from the last layer of the image model to the text "I
      feel {emotion}". This gives us a combination of vectors that we can study.
      We can extract a list of emotions from a feeling wheel, a type of
      hand-crafted user interface for seeing emotions, often hierarchically.
    </p>
    <p>
      Beside the color wheel we can render an Activation Atlas of the
      attribution vectors representing each emotion. To make the atlas more
      legible, we can use non-negative matrix factorization (NMF) on the vectors
      and color each tile based on its factors.
    </p>

    <figure id="emotions-atlas" class="fullscreen-diagram"></figure>

    <p>Okay now we're going to look at some semantic dictionaries.</p>
    <p>theres some interesting clever combos</p>
    <figure id="emotions-semantic-clever" class="fullscreen-diagram"></figure>
    <p>and some related to bias</p>
    <figure id="emotions-semantic-bias" class="fullscreen-diagram"></figure>
    <p>Add a conclusion here for the emotion work.</p>

    <h3>Miscellaneous Neurons</h3>

    <figure id="literate-neurons"></figure>

    <br />
    <hr />
    <br />

    <h2>Typographic Attacks</h2>

    <p>
      As we've shown, it's not just the language side of the multimodal model
      that knows how to read: some features in the vision side appear to
      understand individual letters, numbers, and even full words.
    </p>

    <p>
      These vision features
      <!--are clearly useful if the model needs to do-->
      serve the purpose of allowing the model to perform OCR. Beyond that,
      however, note how the text that is recognized is often closely tied to the
      other concepts that a neuron recognizes. We can see in feature
      visualizations that show both text and imagery: both aspects will gesture
      at the same cluster of concepts. This should be no surprise by now: these
      are multimodal neurons, after all!
    </p>

    <p>
      We've seen that text often makes its way into the model's high-level
      representations, but can the presence or absence of a piece of text
      influence the model's classifications?
    </p>

    <p>
      Surprisingly, we find that it can! simply adding text to an image can
      cause the image to be classified as something it's not. This can be seen
      as a kind of adversarial attack – what we'll call a
      <i>typographic attack</i>.
    </p>

    <figure id="in-the-wild-1" class="fullscreen-diagram"></figure>

    <% let attacks = require('../static/typographic/in_the_wild_1.json') %>

    <p>
      Although we call these a kind of adversarial attack, we are mainly
      interested in what typographic attacks can tell us about the multimodal
      models, rather than seeing them as a competitive adversarial attack. After
      all, they're limited to models with multimodal neurons! Despite this, it
      is helpful to understand how these attacks fit within the broader
      tradition of adversarial attack research.
    </p>

    <p>
      While many adversarial attacks focus on making imperceptible changes to
      images, some attacks instead involve more exotic constraints. Typographic
      attacks are more similar to work in this second line of research,
      including <i>adversarial patches</i
      ><d-cite key="brown2017adversarialpatch"></d-cite> and
      <i>physical adversarial examples</i
      ><d-cite key="athalye2017adversarialturtle"></d-cite>. Adversarial patches
      are stickers that can be placed on a real-life object in order to cause
      neural nets to misclassify that object as something else – for example, a
      toaster. Physical adversarial examples are complete 3D objects that are
      reliabily misclassified: previous work gives a 3D-printed turtle that is
      reliably misclassified as a rifle and a baseball that is misclassified as
      an espresso.
    </p>

    <p>
      How robust are typographic attacks, and can they reliably cause a wide
      variety of images to become misclassified? Answering this using
      <a href="#in-the-wild-1">Figure [[N]]</a>'s physical setting unfortunately
      isn't scalable. To investigate typographic attacks more systematically,
      let's look at them in an automated setting.
    </p>

    <h3>Automating typographic attacks</h3>

    <p>
      We set up a simple automated attack. Each attack consists of an
      <i>attack text</i> that attempts to convert images to a
      <i>target class</i>. We place the text around the image at eight fixed
      coordinates<d-footnote
        >These coordinates aren't particularly special; we choose them just
        because they space the text out around the image.</d-footnote
      >
      and using a fixed font style.
    </p>

    <figure id="attack-setup" class="fullscreen-diagram"></figure>

    <p>
      We apply this attack to images from the ImageNet validation set, and test
      whether the attack is able to reliably switch a large percentage of
      ImageNet validation set images to the target attack class.
    </p>

    <p>We found text snippets for our attacks using a handful of techniques:</p>
    <ol>
      <li>
        Manually looking through the multimodal model's neurons for those that
        appear sensitive to particular kinds of text. This is how we found the
        <i>piggy bank</i> and <i>Siamese cat</i> attacks.
      </li>

      <li>
        Writing a genetic algorithm that modifies text in order to hill-climb
        towards text that is a more effective adversarial attack. This is how we
        found the <i>great white shark</i> and <i>waste container</i> attacks.
      </li>

      <li>
        Just brute-force searching through all of the ImageNet class names
        looking for class names which are, in and of themselves, effective
        attacks. This is how we found <i>rifle</i>, <i>pizza</i>, <i>radio</i>,
        <i>iPod</i>, and <i>library</i>.
      </li>
    </ol>

    <p>
      Under this attack setup, we found several attacks to be reasonably
      effective. Among the strongest of these attacks we see that this simple
      procedure can get us up to a 97% attack success rate with only around 7%
      of the image's pixels changed.
    </p>

    <figure id="automated-attacks" class="l-body-outset"></figure>

    <p>
      To contextualize how effective these results are as an adversarial attack,
      we can compare the linear probes results for the strongest two attacks
      above to the performance of the attacks described in
      <i>Adversarial Patch</i>. For reference, those attacks achieve up to 80%
      attack success at 8% pixel cover – many of the attacks above are as or
      more effective.
    </p>

    <p>
      We can improve upon the results from back in
      <a href="in-the-wild-1">Figure [[N]]</a> by constructing a new dataset of
      physical typographic attacks using some of these more effective attacks
      we've now discovered.
    </p>

    <figure id="in-the-wild-2" class="fullscreen-diagram"></figure>

    <!-- <h4>As a black-box attack</h4>
    <p>
      As a sidenote, one interesting aspect of these typographic attacks is that they may in some cases be effective as <b>few-shot, black-box adversarial attacks</b> – they could plausibly work even in settings where the attacker only has query access to the model.<d-cite
      key="ilyas2018blackbox"></d-cite>

      [[(Second reason, perhaps, but not sure if they are going to be THAT that effective)]] [[(Better to be explicitly humble so that the adversarial attack community knows that's what we're doing here)]]
    </p>

    <p>
      For models vulnerable to this category of attack, an adversary doesn't need to see the inside of the model in order to guess how to attack it: some common words that are likely enough to be related to the ImageNet class may suffice. What's more, the attack can be carried out with only a handful of queries to the model
    </p> -->

    <h3>Why do these attacks work?</h3>

    <p>
      We already know that the multimodal model develops high-level
      representations for concepts, and sometimes those concepts include both
      images and text. Naturally, some of these representations might be
      upstream of the model's representations of the various ImageNet classes.
    </p>

    <figure id="attackable-neurons"></figure>

    <p>
      Above, we see two examples of high-level representations correlated with
      ImageNet classes: a neuron responding to images of piggybanks, money, and
      prices, and a neuron responding to the Apple logo and images and names of
      Apple products.
    </p>

    <p>
      Because these high-level representations are interpretable, we can inspect
      the neurons that the model is using and their weights, and develop
      typographic attacks from there.
    </p>

    <p>
      Inasmuch as the model forms these high level representations, it's forming
      them because those representations <i>efficiently compress</i> the
      training set. And naturally, inputs like the ones in these typographic
      attacks – images spuriously labeled with irrelevant text – are
      <i>not</i> very common in that training set! As a result, a piece of text
      that might normally be a very strong cue may in an attack setting be
      strongly-weighted enough to get in the way of the model's perception of
      other objects elsewhere in the image, simply by outweighing them.
    </p>

    <p>
      The multimodal model's architecture is powerful enough to contain
      representations that learn both images and words, and one effect is what
      we see here: a unique weakness to typographic attacks.
    </p>

    <!-- <p>How else might we investigate just exactly how much text is integrated into the model's higher-level concepts [[edit: duplicative]]? One way we could lower-bound it is if we could show a downstream effect of [[that integration]]. -->

    <!-- In fact, we hypothesize that a fine-tuned model might actually <i>erase</i> this effect – by causing the model to no longer "need" to know how to read. The susceptibility to typographic attacks may well be an interesting consequence of the multimodal model being required to effectively perform at <i>diverse</i> tasks, not just at ImageNet as in these experiments but also at its native image captioning task. -->

    <!-- <p>[[TODO(Experiment+Analysis): Attempt to generate more nuanced analyses by subtracting activations.]]</p> -->

    <!-- <p>TODO(Writing): Gabe's point about the connection to fairness – overgeneralization. [[In the high-level representations. Connects to the model theory point.</p> -->

    <!-- <p>TODO(Writing): Connection to the Stroop effect?</p> -->

    <!--
      Still to-do once I have wifi:
      [] In the Wild 3 – sftp my images to athena.dialup.mit.edu, then run them
      [] Linear probes for In the Wild?? (Did I not have this working before? Huh? What was it that I'm missing here? Maybe I misremember the task?)
      [] Extract out a Svelte component for the image card
      [] Research the Stroop effect and get a couple of citations
      [] (Optional stretch) Extract out a Svelte component for In the Wild
      [] Rewrite the conclusion once I have more brain
    -->


    <br />
    <hr />
    <br />

    <h2>The Mechanics of Abstraction</h2>

    <p>
      One of the greatest mysteries of neural networks is how they can
      accomplish things which no human being knows how to directly write a
      computer program to do. This mystery is present in all non-trivial neural
      networks, but it burns especially brightly in light of a neural network
      which seems to have features previously thought to be signatures of the
      human mind.
    </p>
    <p>
      Recent work analyzing circuits [] has shed some light on the mechanism and
      algorithms running inside the layers of vision models. Building on a
      tradition of researchers studying meaningful neurons inside neural
      networks [], the circuits approach tries to systematically and rigorously
      characterize neurons inside neural networks, and then understand how the
      graph of weights between them implements their behavior. In this section,
      we aim to offer a very preliminary circuit analysis of mulltimodal models.
    </p>
    <div class="todo">
      <b>TODO(@colah)</b>: work in language about being preliminary
    </div>
    <p>
      The abstract features we saw in the previous section only form in the
      final layers of multimodal models. At a high-level, they seem to form by
      unioning over many cases, with every residual layer adding on new cases.
      For example, ...
    </p>

    <figure id="enrichment-diagram" class="fullscreen-diagram"></figure>
    <!--<div class='todo'><b>TODO(colah):</b></div>

    The abstract features we saw in the previous section are produced only after dozens of layers of visual processing -- what happened in those layers?
    -->

    <p>
      We call these circuits union over massive numbers of facets "enrichment
      circuits," and will return to them at the end of this section. But
      enrichment circuits and the abstract features they produce are only
      possible because all the layers up to that point built up a foundation of
      sophisticated features from which the abstract features can be assembled.
      And so if we truly want to understand abstract features, we need to start
      with these.
    </p>

    <h3>Universal Features and Circuits</h3>

    <p>
      As we examined the features and circuits in the early and mid layers of
      the multimodal models, we found many that appeared to be analogues of
      features found in other models. In particular, many of the features we
      found in early vision are strikingly similar to
      <a href="https://distill.pub/2020/circuits/early-vision/"
        >early visual features</a
      >
      of both ImageNet and Places365 models:
    </p>

    <figure id="early-vision" class="fullscreen-diagram"></figure>

    <p>
      Many of these features actually develop multiple times throughout the
      multimodal models we examined, at different scales. For example, we
      consistently see small black and white vs color detectors at the beginning
      of block 2, and then larger ones many layers later at the beginning of
      block 3. We also see interesting differences between the analogues.
    </p>

    <p>
      The idea that analogous features form across models is sometimes called
      the "<a>universality hypothesis</a>." A significant amount of work has
      measured the overall similarity of neural network representations, often
      finding significantly similarity between representations of different
      models []. A smaller amount of work has explored individual analogous
      features forming across models []. Previous demonstrations of universality
      have tended to show that different models trained on the same dataset
      develop similar features, leaving some uncertainty as to whether it would
      be true for models trained on very different datasets. The multimodal
      model provides an example of analogous features forming across models
      which are different in dataset, training objective, and architecture.
    </p>

    <p>
      Not only do multimodal models develop many features which have analogues
      InceptionV1, but we found a number of cases where those features seem to
      be implemented by analogous circuits:
    </p>

    <figure id="universal-circuit-diagram" class="fullscreen-diagram"></figure>
    <div class="placeholder">
      <b>TODO(@colah):</b> Figcaption explaining technical details
    </div>

    <p>
      Finding analogous circuits is a stronger result than finding analogous
      features. You could imagine finding highly-correlated and behaviorally
      similar features across two models, and then realizing they're implemented
      in different ways. (We see different implementations of the same function
      with different algorithms all the time in computer science: QuickSort and
      HeapSort implement the same sort function in different ways!) The fact
      that analogous features are implemented in the same way suggests that
      understanding the mechanisms underlying one model may also help us
      understand other models.
    </p>

    <p>
      We also see features which are analogues in more abstract ways. For
      example, InceptionV1 (ImageNet) has left-oriented and right-oriented head
      detectors which most strongly react to dog heads, which are then merged
      into orientation-invariant head detectors. Multimodal models also have
      left-oriented and right-oriented head detectors, but they most strongly
      react to human heads.<d-footnote
        >InceptionV1 (ImageNet) also has human-specific head detectors, but they
        don't seem to be oriented. Likewise, the multimodal models have
        dog-specific head detectors, but they don't appear to be
        oriented.</d-footnote
      >
      In both cases, the invariant head detectors are created by combining
      pose-specific head detectors.
    </p>

    <figure style="grid-column-start: text; grid-column-end: page">
      <img src="images/head-circuit.png" />
    </figure>

    <h3>Task-specific Features</h3>

    <p>
      In addition to "universal" features which the multimodal models seem to
      share with both ImageNet and Places365 models, we observed features which
      they seemed to share with only one:
    </p>

    <figure id="task-specific" class="fullscreen-diagram"></figure>

    <p>
      There are also features which don't have highly-correlated analogues in
      the ImageNet or Places365 model we studied, but still formed consistently
      across multimodal models. Many of these features seem like something that
      would form in another classification task! In fact, the palm tree
      detectors are very similar to those found in an unpublished visualization
      [] of the geolocalization model PlaNet[].
      <d-footnote
        >We know that the multimodal models can be used for geolocalization, and
        that regional neurons form in the late layers, so it makes sense to
        expect features useful for geolocalization to also exist in lower
        layers.</d-footnote
      >
      Similarly, one imagines that features like the "f"-detector could be found
      in OCR models.
    </p>
    <div class="todo"><b>TODO(@colah)</b>: add facial feature example?</div>

    <p>
      Perhaps the right way to understand mid-level vision in the multimodal is
      as a <em>union</em> over features from a very wide variety of tasks.
    </p>

    <div class="placeholder">
      <b>TODO(@colah, @ggoh)</b>: short section on typograpic features?
    </div>
    <div class="placeholder">
      <b>TODO(@colah, @ggoh)</b>: short section on facial features?
    </div>
    <div class="placeholder">
      <b>TODO(@colah)</b>: weird giant brightness detectors?
    </div>

    <h3>The mechanics of Abstraction</h3>

    <p>
      The middle layers of our models provide a variety of features: detectors
      for various kind of objects, for short words, and much more. But the final
      layers are where the most striking property of multimodal models occurs:
      the formation of abstract, multimodal features.
    </p>

    <p>
      We found circuits in these final layers particularly difficult to analyze.
      The layers have widths of several thousand neurons, depending on the
      model, and there appears to be quite a bit of polysemanticity. However,
      the residual architecture does give us one type of circuit for free: what
      we've called "encrichment circuits," where each residual block adds new
      facets to the abstract neurons.
    </p>

    <figure id="enrichment-diagram-2" class="fullscreen-diagram"></figure>


  </d-article>

  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>We are deeply grateful to... Imaginary Creatures</p>

    <p>Many of our diagrams are based on...</p>

    <h3>Author Contributions</h3>
    <p><b>Research:</b> Alex developed ...</p>

    <p><b>Writing & Diagrams:</b> The text was initially drafted by...</p>

    <h3 style='grid-row: auto / span 3;'>Appendix: Methodological Details</h3>

    <h4>Faceted Feature Visualization</h4>
    
    <p>A neuron is said to have multiple facets if there are multiple distinct cases that they fire for<d-cite key="nguyen2016multifaceted"></d-cite> -- that is, if a feature can be described as <i>OR(case_1, case_2, …)</i>. For example, a pose-invariant dog-head detector that detects dog heads tilted to the left, right, or facing straight on<d-cite key="olah2020zoom"></d-cite>, or a boundary detector which looks for a difference in texture from one side to the other but doesn’t care which is which<d-cite key="olah2020overview"></d-cite>, or a “grocery store” output neuron that detects both exterior and interior views of a grocery store<d-cite key="nguyen2016multifaceted"></d-cite>. (You may also be familiar with the idea of a “polysemantic” neuron<d-cite key="olah2017feature,olah2020zoom"></d-cite>. Polysemantic neurons are a special type of multifaceted neuron in which the facets are unrelated.)</p>

    <p><a href="https://distill.pub/2017/feature-visualization/">Feature visualization</a><d-cite key="erhan2009visualizing,olah2017feature,simonyan2013deep,nguyen2015deep,mordvintsev2015inceptionism,nguyen2016plug"></d-cite> is a technique where the input to a neural network is optimized to create a stimuli demonstrating some behavior, typically maximizing the activation of a neuron.  There are two challenges with using feature visualization on multi-faceted neurons. Firstly, the optimization process may give us an out-of-distribution input that tries to activate multiple facets at once; this can sometimes help reveal the existence of multiple facets, but can also lead to difficult-to-parse or nonsensical stimuli. (We suspect this is true when the multiple facets aren’t mutually inhibiting; for contrast, see mutual inhibition in the InceptionV1 pose invariant dog head circuit<d-cite key="olah2020zoom"></d-cite>.) Secondly, even if the optimization process successfully reveals once facet, it may not reveal others.</p>

    <p>These challenges are of increased significance in the multimodal models we study in this paper, because the highly-abstract neurons at the end of the network seem to activate for an enormous variety of cases!</p>

    <p>We are aware of two past approaches to improving feature visualization for multi-faceted neurons. The first approach is to find highly diverse images which activate a given neuron, and use them as seeds for the feature visualization optimization process<d-cite key="nguyen2016multifaceted"></d-cite>. This approach requires one to have access to the dataset, may taint the value of feature visualization in establishing evidence of causality, and the stimuli may still be pulled into a different basin of attraction. An alternative approach is to perform multiple feature visualizations at once optimizing for an additional “diversity term” which encourages the different feature visualizations to activate different lower-level neurons<d-cite key="olah2017feature"></d-cite>. However, this can incent the visualizations to include unrelated content to activate lower level neurons and increase the diversity term.</p>

    <p>We take a new variant of the second approach, adding a diversity term to the optimization process. However, we base our diversity term not on the <i>activation</i> of lower level neurons, but on the <i>attribution</i> of the high-level neuron to lower level neurons. The intent here is to only incent variation in low-level neurons if it affects the neuron we’re visualizing. To formalize this, we use a simple <i>attribution = gradient ⊙ activation</i> attribution method, which can be seen as the linear approximation of the effect of the lower level neurons on the high level one. Since this is a resnet where there is a linear pathway between them, this seems especially principled. We then maximize the orthogonal components of these attribution vectors . A reference implementation can be found in attached colab notebooks.</p>

    <p>Sometimes it’s desirable to visualize analogous types of facets across many neurons in a controlled manner. In the multimodal models, many neurons have a “text” facet -- in fact, many neurons have lots of different text facets, responding to many words! -- and we’d like to see them for all neurons. To do this, we first collect images of each type (text, faces, etc) and fit a logistic regression. We then add a targeted diversity term <i>regression weights ⊙ attribution</i> to encourage a facet that activates our high-level neuron through low-level neurons associated with a given facet type. Again, a reference implementation can be found in attached colab notebooks.</p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
